{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import zipfile\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a3a36581c8def127",
   "metadata": {},
   "source": [
    "!curl -O https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip\n",
    "\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip\n",
    "zip = zipfile.ZipFile('pizza_steak.zip')\n",
    "zip.extractall()\n",
    "zip.close()\n",
    "zip"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a186731ca691e16b",
   "metadata": {},
   "source": [
    "import os\n",
    "for dirpath, dirnames, filenames in os.walk('pizza_steak'):\n",
    "    print(f\"there are {len(dirnames)} directories and {len(filenames)} images in {dirpath}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93a4ae300da932a",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5a1780f-8380-42cc-92d8-b2f8dfa9f43b",
   "metadata": {},
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "data_dir=pathlib.Path('pizza_steak/train')\n",
    "class_names=np.array(sorted([item.name for item in data_dir.glob(\"*\")]))\n",
    "print(class_names)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "import os\n",
    "\n",
    "def view_random_image(target_dir, target_class):\n",
    "    target_folder = target_dir + target_class\n",
    "    random_image = random.sample(os.listdir(target_folder), 1)\n",
    "    print(random_image)\n",
    "    img = mpimg.imread(target_folder + \"/\" + random_image[0])\n",
    "    plt.imshow(img)\n",
    "    plt.title(target_class)\n",
    "    plt.axis('off')\n",
    "    print(f\"image shape: {img.shape}\")\n",
    "    return img"
   ],
   "id": "6d908db61a9b9fd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "img=view_random_image(target_dir=\"pizza_steak/train/\", target_class=\"steak\")",
   "id": "56d77799250c3bcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tf.constant(img)",
   "id": "50184fad2a7c0d10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.api.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras.api.models import Sequential\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_dir = \"/Artificial/TENSORFLOW/pizza_steak/train/\"\n",
    "test_dir = \"/Artificial/TENSORFLOW/pizza_steak/test/\"\n",
    "\n",
    "#import data frm directories and turn it into batches\n",
    "train_data = train_datagen.flow_from_directory(train_dir,\n",
    "                                               batch_size=32,\n",
    "                                               target_size=(224, 224),\n",
    "                                               class_mode=\"binary\",\n",
    "                                               seed=42)\n",
    "valid_data = valid_datagen.flow_from_directory(test_dir,\n",
    "                                               batch_size=32,\n",
    "                                               target_size=(224, 224),\n",
    "                                               class_mode=\"binary\",\n",
    "                                               seed=42)\n",
    "\n",
    "model1 = Sequential([\n",
    "    Conv2D(filters=10, kernel_size=3, activation='relu', input_shape=(224, 224, 3)),\n",
    "    Conv2D(filters=10, kernel_size=3, activation='relu'),\n",
    "    MaxPooling2D(pool_size=2, padding='valid'),\n",
    "    Conv2D(filters=10, kernel_size=3, activation='relu'),\n",
    "    Conv2D(filters=10, kernel_size=3, activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "his1 = model1.fit(train_data, epochs=5, steps_per_epoch=len(train_data),\n",
    "                  validation_data=valid_data, validation_steps=len(valid_data), verbose=2)"
   ],
   "id": "6b712b7b3206d26e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model1.summary()",
   "id": "de2e6d8dbbf5e314",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_dir = \"/Artificial/TENSORFLOW/pizza_steak/train/\"\n",
    "test_dir = \"/Artificial/TENSORFLOW/pizza_steak/test/\"\n",
    "\n",
    "#import data frm directories and turn it into batches\n",
    "train_data = train_datagen.flow_from_directory(train_dir,\n",
    "                                               batch_size=32,\n",
    "                                               target_size=(224, 224),\n",
    "                                               class_mode=\"binary\",\n",
    "                                               seed=42)\n",
    "valid_data = valid_datagen.flow_from_directory(test_dir,\n",
    "                                               batch_size=32,\n",
    "                                               target_size=(224, 224),\n",
    "                                               class_mode=\"binary\",\n",
    "                                               seed=42)"
   ],
   "id": "8e4c92985622e57d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras.api.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras.api.models import Sequential\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model2=Sequential([\n",
    "    Flatten(input_shape=(224, 224, 3)),\n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "his2=model2.fit(train_data, epochs=5, steps_per_epoch=len(train_data),\n",
    "                validation_data=valid_data, validation_steps=len(valid_data), verbose=2)"
   ],
   "id": "bf50a6f1aa4f2fce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model2.summary()",
   "id": "eb3ad9aa559b708f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tf.random.set_seed(42)\n",
    "model3 = Sequential([\n",
    "    Flatten(input_shape=(224, 224, 3)),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "his3 = model3.fit(train_data, epochs=5, steps_per_epoch=len(train_data), validation_data=valid_data,\n",
    "                  validation_steps=len(valid_data), verbose=2)"
   ],
   "id": "609ee539bbe8cd34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "steak_img=view_random_image(\"pizza_steak/train/\", \"steak\")\n",
    "plt.subplot(1,2,2)\n",
    "pizza_img=view_random_image(\"pizza_steak/test/\", \"pizza\")"
   ],
   "id": "843cdac494b90e23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Preproccessing data\n",
    "\n",
    "train_dir = \"/Artificial/TENSORFLOW/pizza_steak/train/\"\n",
    "test_dir = \"/Artificial/TENSORFLOW/pizza_steak/test/\"\n",
    "\n",
    "#Into batches\n",
    "\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "train_data = train_datagen.flow_from_directory(train_dir,\n",
    "                                               target_size=(224, 224),\n",
    "                                               class_mode=\"binary\",\n",
    "                                               batch_size=32, )\n",
    "test_data = test_datagen.flow_from_directory(test_dir,\n",
    "                                             target_size=(224, 224),\n",
    "                                             class_mode=\"binary\",\n",
    "                                             batch_size=32)"
   ],
   "id": "9e70882de6caac28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras.api.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.optimizers import Adam\n",
    "from keras.api.losses import binary_crossentropy\n",
    "\n",
    "model4 = Sequential([\n",
    "    Conv2D(filters=10,\n",
    "           kernel_size=(3,3),\n",
    "           strides=(1,1),\n",
    "           padding='valid',\n",
    "           activation='relu',\n",
    "           input_shape=(224, 224, 3)),  # input layer (specify input shape)\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model4.compile(optimizer=Adam(), loss=binary_crossentropy, metrics=['accuracy'])\n",
    "his4 = model4.fit(train_data, epochs=5, steps_per_epoch=len(train_data),\n",
    "                  validation_data=valid_data, validation_steps=len(valid_data), verbose=2)"
   ],
   "id": "ab29b32d0a388d27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.DataFrame(his4.history).plot(figsize=(10, 5))"
   ],
   "id": "5d8a0cc0ed498a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def  plot_loss_curves(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    epoch=range(len(history.history['loss']))\n",
    "\n",
    "#loss\n",
    "    plt.plot(epoch,loss,label='train loss')\n",
    "    plt.plot(epoch,val_loss,label='val loss')\n",
    "    plt.title('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "\n",
    "#accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(epoch,accuracy,label='train accuracy')\n",
    "    plt.plot(epoch,val_acc,label='val accuracy')\n",
    "    plt.title('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()"
   ],
   "id": "e3f6208651ddb57a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_loss_curves(his4)",
   "id": "c9eb8b6cc0fc9c60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras.api.layers import MaxPool2D\n",
    "\n",
    "#Adjust overfitting\n",
    "\n",
    "model5 = Sequential([\n",
    "    Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPool2D(pool_size=2),\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model5.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "his5 = model5.fit(train_data, epochs=5, steps_per_epoch=len(train_data), validation_data=valid_data,\n",
    "                  validation_steps=len(valid_data), verbose=2)"
   ],
   "id": "dee10cf4d4f497ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model5.summary()",
   "id": "7509d8f5b04069da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_loss_curves(his5)",
   "id": "aee014a0fe452749",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_datagen_augmented = ImageDataGenerator(rescale=1/255.,\n",
    "                                             rotation_range=20, # rotate the image slightly between 0 and 20 degrees (note: this is an int not a float)\n",
    "                                             shear_range=0.2, # shear the image\n",
    "                                             zoom_range=0.2, # zoom into the image\n",
    "                                             width_shift_range=0.2, # shift the image width ways\n",
    "                                             height_shift_range=0.2, # shift the image height ways\n",
    "                                             horizontal_flip=True) # flip the image on the horizontal axis\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1/255.)"
   ],
   "id": "6c61fa4fb00cffd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Augmented training images:\")\n",
    "train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,\n",
    "                                                                   target_size=(224, 224),\n",
    "                                                                   batch_size=32,\n",
    "                                                                   class_mode='binary',\n",
    "                                                                   shuffle=False)\n",
    "\n",
    "# Create non-augmented data batches\n",
    "print(\"Non-augmented training images:\")\n",
    "train_data = train_datagen.flow_from_directory(train_dir,\n",
    "                                               target_size=(224, 224),\n",
    "                                               batch_size=32,\n",
    "                                               class_mode='binary',\n",
    "                                               shuffle=False)\n",
    "\n",
    "print(\"Unchanged test images:\")\n",
    "test_data = test_datagen.flow_from_directory(test_dir,\n",
    "                                             target_size=(224, 224),\n",
    "                                             batch_size=32,\n",
    "                                             class_mode='binary')"
   ],
   "id": "72ffe154585161eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "images, labels = train_data.next()\n",
    "augmented_images, augmented_labels = train_data_augmented.next()\n",
    "random_number = random.randint(0, 31) # we're making batches of size 32, so we'll get a random instance\n",
    "plt.imshow(images[random_number])\n",
    "plt.title(f\"Original image\")\n",
    "plt.axis(False)\n",
    "plt.figure()\n",
    "plt.imshow(augmented_images[random_number])\n",
    "plt.title(f\"Augmented image\")\n",
    "plt.axis(False);"
   ],
   "id": "18fc3edb4235fb2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model6 = Sequential([\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    MaxPool2D(pool_size=2),\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model6.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "hits6 = model6.fit(train_data_augmented,\n",
    "                   epochs=5,\n",
    "                   steps_per_epoch=len(train_data_augmented),\n",
    "                   validation_data=test_data,\n",
    "                   validation_steps=len(test_data), verbose=2)"
   ],
   "id": "c391dac3c1c106e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_loss_curves(hits6)",
   "id": "f467f900dc946216",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_data_augmented_shuffled = train_datagen_augmented.flow_from_directory(train_dir,\n",
    "                                                                            target_size=(224, 224),\n",
    "                                                                            batch_size=32,\n",
    "                                                                            class_mode='binary',\n",
    "                                                                            shuffle=True)"
   ],
   "id": "184dc712e6fe248f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model7 = Sequential([\n",
    "    Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model7.compile(loss='binary_crossentropy',\n",
    "               optimizer=Adam(),\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "his7 = model7.fit(train_data_augmented_shuffled,\n",
    "                  epochs=5,\n",
    "                  steps_per_epoch=len(train_data_augmented_shuffled),\n",
    "                  validation_data=test_data,\n",
    "                  validation_steps=len(test_data), verbose=2)"
   ],
   "id": "30e286824058d970",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_loss_curves(his7)",
   "id": "2fb31bfd0c908cb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!curl -O https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg\n",
    "steak = mpimg.imread(\"03-steak.jpeg\")\n",
    "plt.imshow(steak)\n",
    "plt.axis(False);"
   ],
   "id": "5b1102fa05939ac9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import zipfile\n",
    "\n",
    "!curl -O https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\n",
    "\n",
    "# Unzip the downloaded file\n",
    "zip_ref = zipfile.ZipFile(\"10_food_classes_all_data.zip\", \"r\")\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ],
   "id": "167392d56cb75844",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Walk through 10_food_classes directory and list number of files\n",
    "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_all_data\"):\n",
    "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ],
   "id": "f59595f6da4407b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dir = \"10_food_classes_all_data/train/\"\n",
    "test_dir = \"10_food_classes_all_data/test/\""
   ],
   "id": "b8ee180a70edc6eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "data_dir = pathlib.Path(train_dir)\n",
    "class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))\n",
    "print(class_names)"
   ],
   "id": "4e24615e7ae2cc98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "img = view_random_image(target_dir=train_dir,\n",
    "                        target_class=random.choice(class_names))"
   ],
   "id": "463b35b5b24699b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Rescale the data and create data generator instances\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "# Load data in from directories and turn it into batches\n",
    "train_data = train_datagen.flow_from_directory(train_dir,\n",
    "                                               target_size=(224, 224),\n",
    "                                               batch_size=32,\n",
    "                                               class_mode='categorical') # changed to categorical\n",
    "\n",
    "test_data = train_datagen.flow_from_directory(test_dir,\n",
    "                                              target_size=(224, 224),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='categorical')"
   ],
   "id": "5fa82cffa17c2108",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from keras.api.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_1.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history1 = model_1.fit(train_data,\n",
    "                       epochs=5,\n",
    "                       steps_per_epoch=len(train_data),\n",
    "                       validation_data=test_data,\n",
    "                       validation_steps=len(test_data), verbose=2)"
   ],
   "id": "31b7c361ff64f7de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_loss_curves(history1)\n",
   "id": "f98e49702ba64a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_datagen_augmented = ImageDataGenerator(rescale=1/255.,\n",
    "                                             rotation_range=20,\n",
    "                                             width_shift_range=0.2,\n",
    "                                             height_shift_range=0.2,\n",
    "                                             zoom_range=0.2,\n",
    "                                             horizontal_flip=True)\n",
    "\n",
    "train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,\n",
    "                                                                  target_size=(224, 224),\n",
    "                                                                  batch_size=32,\n",
    "                                                                  class_mode='categorical')"
   ],
   "id": "7361d1940ba4688f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_2 = tf.keras.models.clone_model(model_1)\n",
    "\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history_2 = model_2.fit(train_data_augmented,\n",
    "                         epochs=5,\n",
    "                         steps_per_epoch=len(train_data_augmented),\n",
    "                         validation_data=test_data,\n",
    "                         validation_steps=len(test_data), verbose=2)\n"
   ],
   "id": "bb394ec5b53909c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_loss_curves(history_2)",
   "id": "c0e239ddd07cb8e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!curl -O -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg\n",
    "!curl -O -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg\n",
    "!curl -O -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-hamburger.jpeg\n",
    "!curl -O -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-sushi.jpeg"
   ],
   "id": "ada5c694683932ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_and_prep_image(filename, img_shape=224):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.resize(img, size=[img_shape, img_shape])\n",
    "\n",
    "    img = img / 255.\n",
    "    return img\n",
    "\n",
    "\n",
    "def pred_and_plot(model, filename, class_names):\n",
    "    \"\"\"\n",
    "    Imports an image located at filename, makes a prediction on it with\n",
    "    a trained model and plots the image with the predicted class as the title.\n",
    "    \"\"\"\n",
    "    # Import the target image and preprocess it\n",
    "    img = load_and_prep_image(filename)\n",
    "\n",
    "    # Make a prediction\n",
    "    pred = model.predict(tf.expand_dims(img, axis=0))\n",
    "\n",
    "    # Get the predicted class\n",
    "    pred_class = class_names[int(tf.round(pred)[0][0])]\n",
    "\n",
    "    # Plot the image and predicted class\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Prediction: {pred_class}\")\n",
    "    plt.axis(False);\n",
    "\n",
    "\n",
    "# Load in and preprocess our custom image\n",
    "img = load_and_prep_image(\"03-steak.jpeg\")\n",
    "\n",
    "# Make a prediction\n",
    "pred = model_2.predict(tf.expand_dims(img, axis=0))\n",
    "\n",
    "# Match the prediction class to the highest prediction probability\n",
    "pred_class = class_names[pred.argmax()]\n",
    "plt.imshow(img)\n",
    "plt.title(pred_class)\n",
    "plt.axis(False);"
   ],
   "id": "36722793ac9a9810",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pred = model_2.predict(tf.expand_dims(img, axis=0))\n",
    "pred"
   ],
   "id": "2a89ca9589e020a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8dfff5e26c0fa3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e043c3d30517362f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
