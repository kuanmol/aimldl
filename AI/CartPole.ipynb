{
 "cells": [
  {
   "cell_type": "code",
   "id": "d60bdb9f0b0b9115",
   "metadata": {},
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]  # 4\n",
    "action_size = env.action_space.n\n",
    "state_size, action_size\n"
   ],
   "id": "b5a5b1a7008d5f00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)"
   ],
   "id": "3fecbfa181cf46e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What is a Replay Buffer?\n",
    "\n",
    "In Reinforcement Learning (RL), especially DQN, we use a Replay Buffer to store what the agent sees and does, and then train from that stored experience rather than only the latest step.\n",
    "\n",
    "This makes learning more stable and efficient.\n",
    "\n",
    "**Buffer_size**: Maximum number of experiences the buffer can hold (e.g., 10,000).\n",
    "\n",
    "**Batch_size**: Number of experiences to pull out when it's time to learn (e.g., 64)."
   ],
   "id": "90e7759f21dcdf6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size  # <-- Add this line\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append(\n",
    "            (state, action, reward, next_state, done))  #This method adds a new experience into the buffer.\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        #This chunk is converting raw experience data into PyTorch tensors for training.\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.float32)).float().to(device)\n",
    "        return (states, actions, rewards, next_states,\n",
    "                dones)  #Returns a tuple of tensors that the agent can use to learn (Q-values).\n",
    "\n",
    "    def __len__(self):  #This lets you call len(buffer) to check how many experiences are stored.\n",
    "        return len(self.memory)\n",
    "\n",
    "#Purpose: You're setting up the memory space and how big each learning bite will be.\n",
    "# Why random?\n",
    "#    To break correlation between steps.\n",
    "#    Prevents overfitting to recent events."
   ],
   "id": "38522a9d0abc7727",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Class in Deep Q-Learning (DQN)\n",
    "This class defines a DQN agent that interacts with the environment, stores experiences, and learns from them using two neural networks (local and target).\n",
    "\n",
    "- Local network is used to choose actions.\n",
    "\n",
    "- Target network is used to compute stable targets for learning.\n",
    "\n",
    "**gamma** :\tDiscount factor for future rewards\n",
    "\n",
    "**lr** :\tLearning rate for optimizer\n",
    "\n",
    "**tau** :\tFactor for soft updating target network\n",
    "\n",
    "### This is the Œµ-greedy strategy, a key part of balancing exploration and exploitation.\n",
    "\n",
    "- random.random() generates a number between 0 and 1.\n",
    "\n",
    "- If that number is greater than epsilon, the agent chooses the best action using the network.\n",
    "\n",
    "- So the probability of choosing the best action is 1 - epsilon.\n",
    "\n",
    "- Otherwise, it chooses a random action to explore."
   ],
   "id": "33c57eb291b78f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, buffer_size=10000, batch_size=64, gamma=0.99, lr=1e-3, tau=1e-2):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
    "        self.memory = ReplayBuffer(buffer_size, batch_size)\n",
    "        self.t_steps = 0\n",
    "        self.update_every = 4  # Learn every 4 steps\n",
    "\n",
    "\n",
    "    def act(self, state, epsilon=0.0):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "        #Evaluate the model without training it (no gradient tracking).\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            q_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #With probability 1 - epsilon, take the best action. Otherwise, choose a random action (for exploration).\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(q_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return np.random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.t_steps = (self.t_steps + 1) % self.batch_size\n",
    "        if self.t_steps % self.update_every == 0 and len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        #Use the target network to get the best Q-value for next state.\n",
    "        #detach() to avoid gradients from flowing through target net\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        #Get Q-values from local net for the actions taken.\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target)\n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "    #This blends the parameters of the local model into the target model using soft updates\n",
    "    # Œ∏_target = œÑ * Œ∏_local + (1 - œÑ) * Œ∏_target\n"
   ],
   "id": "30a7f34f31b63b54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_episodes = 500\n",
    "max_t = 1000\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "scores = []\n",
    "scores_window = deque(maxlen=100)\n",
    "agent = Agent(state_size, action_size)"
   ],
   "id": "2df2804b848c6ff6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for episode in range(1, n_episodes + 1):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(max_t):\n",
    "        action = agent.act(state, epsilon)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Modified reward to encourage longer episodes\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = -10.0\n",
    "\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    scores.append(total_reward)\n",
    "    scores_window.append(total_reward)\n",
    "    epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
    "\n",
    "    print(f\"Episode {episode}, Score: {total_reward}, Avg100: {np.mean(scores_window):.2f}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    if np.mean(scores_window) >= 195.0:\n",
    "        print(f\"\\nEnvironment solved in {episode} episodes!\")\n",
    "        torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "        break\n",
    "\n",
    "env.close()"
   ],
   "id": "2e7e2baa3665f92b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import base64\n",
    "import imageio\n",
    "import os\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def show_video_of_model(agent, env_name=\"CartPole-v1\", output_path=\"video.mp4\"):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        action = agent.act(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Save video\n",
    "    imageio.mimsave(output_path, frames, fps=30)\n",
    "    print(f\"‚úÖ Video saved to: {os.path.abspath(output_path)}\")\n",
    "\n",
    "def show_video(video_path=\"video.mp4\"):\n",
    "    if os.path.exists(video_path):\n",
    "        with open(video_path, 'rb') as f:\n",
    "            video = f.read()\n",
    "        encoded = base64.b64encode(video).decode('ascii')\n",
    "        display(HTML(f'''\n",
    "            <video alt=\"test\" autoplay loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\" />\n",
    "            </video>'''))\n",
    "    else:\n",
    "        print(f\"‚ùå Could not find video at {video_path}\")\n",
    "\n",
    "# üîΩ Example usage:\n",
    "output_file = \"D:/Artificial/AI/cartpole_trained.mp4\"\n",
    "show_video_of_model(agent, env_name=\"CartPole-v1\", output_path=output_file)\n",
    "show_video(output_file)\n"
   ],
   "id": "a789f75d3b44961b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
