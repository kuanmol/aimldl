{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device for torch computations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "id": "3129e6560151d145",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ],
   "id": "d4e0422ce7c14198",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.FloatTensor(np.vstack([e[0] for e in experiences])).to(device)\n",
    "        actions = torch.LongTensor(np.vstack([e[1] for e in experiences])).to(device)\n",
    "        rewards = torch.FloatTensor(np.vstack([e[2] for e in experiences])).to(device)\n",
    "        next_states = torch.FloatTensor(np.vstack([e[3] for e in experiences])).to(device)\n",
    "        dones = torch.FloatTensor(np.vstack([e[4] for e in experiences])).to(device)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ],
   "id": "c3f4bb789672dba1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, buffer_size=int(1e5), batch_size=128,\n",
    "                 gamma=0.99, lr=1e-3, tau=1e-3, update_every=4):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.update_every = update_every\n",
    "        self.t_step = 0\n",
    "\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
    "        self.memory = ReplayBuffer(buffer_size, batch_size)\n",
    "\n",
    "    def act(self, state, epsilon=0.0):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0 and len(self.memory) >= self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Obtain maximum predicted Q values (for next states) from the target network\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states using the Bellman equation\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "        # Get expected Q values from the local network\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.soft_update()\n",
    "\n",
    "    def soft_update(self):\n",
    "        # Soft update model parameters:\n",
    "        # θ_target = τ*θ_local + (1 − τ)*θ_target\n",
    "        for target_param, local_param in zip(self.qnetwork_target.parameters(), self.qnetwork_local.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)"
   ],
   "id": "39216e6dcad3825f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def shaped_reward(next_state, terminated):\n",
    "    # Reconstruct the angles from cosine and sine components\n",
    "    theta1 = np.arctan2(next_state[1], next_state[0])\n",
    "    theta2 = np.arctan2(next_state[3], next_state[2])\n",
    "    # Compute tip height (approximately in the range [-2, 0])\n",
    "    height = -np.cos(theta1) - np.cos(theta1 + theta2)\n",
    "    # Normalize height so that 0 corresponds to lowest and 1 corresponds to highest\n",
    "    height_norm = (height + 2) / 2\n",
    "\n",
    "    base = -1.0\n",
    "    height_reward = 2.0 * height_norm\n",
    "    vel1 = next_state[4]\n",
    "    vel2 = next_state[5]\n",
    "    velocity_penalty = -0.1 * (abs(vel1) + abs(vel2))\n",
    "    success_bonus = 100.0 if terminated else 0.0\n",
    "\n",
    "    # Standstill bonus if near the top (height_norm > 0.95) and very low angular velocities\n",
    "    if height_norm > 0.95 and abs(vel1) < 0.05 and abs(vel2) < 0.05:\n",
    "        stillness_bonus = 50.0\n",
    "    else:\n",
    "        stillness_bonus = 0.0\n",
    "\n",
    "    time_penalty = 0.0  # We already have base reward -1 per step\n",
    "\n",
    "    return base + height_reward + velocity_penalty + success_bonus + stillness_bonus + time_penalty\n"
   ],
   "id": "9ccb8f0427a836a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def train_agent(agent, env, n_episodes=2000, max_t=500, eps_start=1.0, eps_end=0.05, eps_decay=0.995):\n",
    "    scores = []  # Will store cumulative environment rewards (-1 per step until termination)\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "\n",
    "    for episode in range(1, n_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        score = 0  # Raw environment reward; note: each step gives -1 unless terminated early.\n",
    "\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, env_reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Compute shaped reward (which now includes a standstill bonus)\n",
    "            reward = shaped_reward(next_state, terminated)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            score += env_reward  # accumulate raw environment reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores.append(score)\n",
    "        scores_window.append(score)\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "        print(f'\\rEpisode {episode}\\tAverage Env Reward: {np.mean(scores_window):.2f}', end='')\n",
    "        if episode % 100 == 0:\n",
    "            print(f'\\rEpisode {episode}\\tAverage Env Reward: {np.mean(scores_window):.2f}')\n",
    "\n",
    "        # Acrobot-v1 is \"solved\" if episodes end early (e.g. around -100 vs. -500)\n",
    "        if np.mean(scores_window) >= -100:\n",
    "            print(f'\\nEnvironment solved in {episode} episodes!\\tAverage Env Reward: {np.mean(scores_window):.2f}')\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'acrobot_solved.pth')\n",
    "            break\n",
    "\n",
    "    return scores\n"
   ],
   "id": "922960a09a8cbc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('Acrobot-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # Create our DQN agent\n",
    "    agent = Agent(state_size, action_size)\n",
    "\n",
    "    # Train the agent\n",
    "    scores = train_agent(agent, env)\n",
    "\n",
    "    # Plot the training progress (environment rewards)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(scores, label='Episode Score (Env Reward)')\n",
    "    running_avg = [np.mean(scores[max(0, i - 100):i + 1]) for i in range(len(scores))]\n",
    "    plt.plot(running_avg, label='Running Average (100 eps)', linestyle='--')\n",
    "    plt.title('Training Progress on Acrobot-v1')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.ylabel('Score (Env Reward)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    env.close()\n"
   ],
   "id": "40ce817ee9c47781",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import base64\n",
    "import imageio\n",
    "import gym\n",
    "import torch\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def show_video_of_model(agent, env_name=\"Acrobot-v1\", output_path=\"acrobot_video.mp4\"):\n",
    "\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        action = agent.act(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Save the video using imageio.\n",
    "    imageio.mimsave(output_path, frames, fps=30)\n",
    "    print(f\"✅ Video saved to: {os.path.abspath(output_path)}\")\n",
    "\n",
    "\n",
    "def show_video(video_path=\"acrobot_video.mp4\"):\n",
    "\n",
    "    if os.path.exists(video_path):\n",
    "        with open(video_path, \"rb\") as f:\n",
    "            video = f.read()\n",
    "        encoded = base64.b64encode(video).decode(\"ascii\")\n",
    "        display(HTML(f\"\"\"\n",
    "        <video autoplay loop controls style=\"height: 400px;\">\n",
    "            <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\" />\n",
    "        </video>\n",
    "        \"\"\"))\n",
    "    else:\n",
    "        print(\"❌ No video found.\")\n",
    "\n",
    "\n",
    "agent.qnetwork_local.load_state_dict(torch.load(\"acrobot_solved.pth\", map_location=torch.device(\"cpu\")))\n",
    "show_video_of_model(agent, env_name=\"Acrobot-v1\", output_path=\"acrobot_video.mp4\")\n",
    "show_video(\"acrobot_video.mp4\")\n"
   ],
   "id": "989c4ad71412ce98",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
