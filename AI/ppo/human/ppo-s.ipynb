{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-21T08:54:04.521325Z",
     "start_time": "2025-04-21T08:45:11.631156Z"
    }
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "import cloudpickle\n",
    "\n",
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPSILON = 0.2\n",
    "ACTOR_LR = 3e-5\n",
    "CRITIC_LR = 1e-4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1024\n",
    "ENTROPY_COEF = 0.1\n",
    "CONTROL_PENALTY = 0.0\n",
    "GRAD_CLIP = 0.5\n",
    "MIN_STD = 1e-6\n",
    "MAX_EPISODE_STEPS = 1000\n",
    "ENV_NAME = \"HumanoidStandup-v4\"\n",
    "TOTAL_TIMESTEPS = 20000000\n",
    "REWARD_THRESHOLD = 45000\n",
    "THRESHOLD_EPISODES = 3\n",
    "PLATEAU_WINDOW = 50\n",
    "PLATEAU_IMPROVEMENT = 0.005\n",
    "EVAL_EPISODES = 5\n",
    "EVAL_MAX_STEPS = 2000\n",
    "EVAL_SAVE_VIDEO = True\n",
    "\n",
    "\n",
    "class RunningStat:\n",
    "    def __init__(self, shape):\n",
    "        self.mean = np.zeros(shape, dtype=np.float32)\n",
    "        self.std = np.ones(shape, dtype=np.float32)\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_std = np.std(x, axis=0) + MIN_STD\n",
    "        batch_count = x.shape[0]\n",
    "        self.count += batch_count\n",
    "        delta = batch_mean - self.mean\n",
    "        self.mean += delta * batch_count / self.count\n",
    "        m_a = self.std * self.std * (self.count - batch_count)\n",
    "        m_b = batch_std * batch_std * batch_count\n",
    "        self.std = np.sqrt((m_a + m_b + np.square(delta) * self.count * batch_count / self.count) / self.count)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        normalized = (x - self.mean) / self.std\n",
    "        return np.clip(normalized, -10, 10)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, act_dim * 2)\n",
    "        )\n",
    "        nn.init.orthogonal_(self.net[-1].weight, gain=0.01)\n",
    "        nn.init.zeros_(self.net[-1].bias)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.net(state)\n",
    "        mean, log_std = x.chunk(2, dim=-1)\n",
    "        log_std = torch.clamp(log_std, -10, 2)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        if torch.isnan(mean).any() or torch.isnan(log_std).any():\n",
    "            return None, None, None\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mean, std)\n",
    "        pre_tanh = dist.rsample()\n",
    "        action = torch.tanh(pre_tanh)\n",
    "        log_prob = dist.log_prob(pre_tanh) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(dim=-1)\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "        return action, log_prob, entropy\n",
    "\n",
    "    def get_action(self, state, deterministic=False):\n",
    "        mean, log_std = self.forward(state)\n",
    "        if deterministic:\n",
    "            return torch.tanh(mean)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mean, std)\n",
    "        pre_tanh = dist.sample()\n",
    "        return torch.tanh(pre_tanh)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        nn.init.orthogonal_(self.net[-1].weight, gain=1.0)\n",
    "        nn.init.zeros_(self.net[-1].bias)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, obs_dim, act_dim, device):\n",
    "        self.actor = Actor(obs_dim, act_dim).to(device)\n",
    "        self.critic = Critic(obs_dim).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=ACTOR_LR)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=CRITIC_LR)\n",
    "        self.device = device\n",
    "        self.obs_normalizer = RunningStat(obs_dim)\n",
    "\n",
    "    def compute_gae(self, rewards, values, next_value, dones):\n",
    "        advantages = np.zeros_like(rewards)\n",
    "        gae = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + GAMMA * next_value * (1 - dones[t]) - values[t]\n",
    "            gae = delta + GAMMA * LAMBDA * (1 - dones[t]) * gae\n",
    "            advantages[t] = gae\n",
    "            next_value = values[t]\n",
    "        returns = advantages + values\n",
    "        return advantages, returns\n",
    "\n",
    "    def update(self, states, actions, old_log_probs, returns, advantages):\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        old_log_probs = torch.FloatTensor(old_log_probs).to(self.device)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages = torch.FloatTensor(advantages).to(self.device)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            # Shuffle indices for mini-batches\n",
    "            indices = np.random.permutation(len(states))\n",
    "            for start in range(0, len(states), BATCH_SIZE):\n",
    "                batch_idx = indices[start:start + BATCH_SIZE]\n",
    "                batch_states = states[batch_idx]\n",
    "                batch_actions = actions[batch_idx]\n",
    "                batch_old_log_probs = old_log_probs[batch_idx]\n",
    "                batch_returns = returns[batch_idx]\n",
    "                batch_advantages = advantages[batch_idx]\n",
    "\n",
    "                # Actor update\n",
    "                action_out, log_prob, entropy = self.actor.sample(batch_states)\n",
    "                if action_out is None:\n",
    "                    continue\n",
    "                ratio = torch.exp(log_prob - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - CLIP_EPSILON, 1 + CLIP_EPSILON) * batch_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean() - ENTROPY_COEF * entropy.mean()\n",
    "\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), GRAD_CLIP)\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                # Critic update (compute fresh value to avoid graph issues)\n",
    "                value = self.critic(batch_states).squeeze()\n",
    "                critic_loss = (batch_returns - value).pow(2).mean()\n",
    "\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), GRAD_CLIP)\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, 'wb') as f:\n",
    "            cloudpickle.dump(self, f)\n",
    "\n",
    "    def evaluate(self, env, episodes=EVAL_EPISODES, max_steps=EVAL_MAX_STEPS, save_video=EVAL_SAVE_VIDEO):\n",
    "        print(\"\\nStarting evaluation...\")\n",
    "        total_reward = 0\n",
    "        standing_episodes = 0\n",
    "        height_threshold = 1.1\n",
    "\n",
    "        if save_video:\n",
    "            video_folder = \"evaluation_videos\"\n",
    "            os.makedirs(video_folder, exist_ok=True)\n",
    "            video_writer = None\n",
    "\n",
    "        for ep in range(episodes):\n",
    "            obs, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            done = False\n",
    "            max_height = 0\n",
    "\n",
    "            if save_video:\n",
    "                video_path = f\"{video_folder}/eval_episode_{ep + 1}.mp4\"\n",
    "                frame = env.render()\n",
    "                if frame is not None:\n",
    "                    video_writer = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), 30,\n",
    "                                                   (frame.shape[1], frame.shape[0]))\n",
    "                else:\n",
    "                    print(\"Warning: Render returned None, skipping video\")\n",
    "\n",
    "            while not done and steps < max_steps:\n",
    "                norm_obs = self.obs_normalizer.normalize(obs)\n",
    "                obs_tensor = torch.FloatTensor(norm_obs).to(self.device).unsqueeze(0)\n",
    "                action, _, _ = self.actor.sample(obs_tensor)\n",
    "                if action is None:\n",
    "                    print(\"Warning: Invalid action, breaking episode\")\n",
    "                    break\n",
    "                action_np = action.detach().cpu().numpy()[0] * 0.4  # Scale actions to [-0.4, 0.4]\n",
    "\n",
    "                obs, reward, terminated, truncated, info = env.step(action_np)\n",
    "                done = terminated or truncated\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "                height = obs[2]\n",
    "                max_height = max(max_height, height)\n",
    "\n",
    "                if save_video and video_writer is not None:\n",
    "                    frame = env.render()\n",
    "                    if frame is not None:\n",
    "                        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                        video_writer.write(frame)\n",
    "\n",
    "            total_reward += episode_reward\n",
    "            if max_height > height_threshold:\n",
    "                standing_episodes += 1\n",
    "\n",
    "            print(f\"Evaluation Episode {ep + 1}: Reward: {episode_reward:.2f}, Max Height: {max_height:.2f}\")\n",
    "            if save_video and video_writer is not None:\n",
    "                video_writer.release()\n",
    "\n",
    "        avg_reward = total_reward / episodes\n",
    "        standing_ratio = standing_episodes / episodes\n",
    "        print(f\"Average Evaluation Reward: {avg_reward:.2f}\")\n",
    "        print(f\"Standing Success Rate: {standing_ratio:.2f} ({standing_episodes}/{episodes} episodes)\")\n",
    "        return avg_reward, standing_ratio\n",
    "\n",
    "\n",
    "def train():\n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agent = PPO(obs_dim, act_dim, device)\n",
    "\n",
    "    total_steps = 0\n",
    "    episode_rewards = []\n",
    "    threshold_count = 0\n",
    "    plateau_rewards = []\n",
    "\n",
    "    while total_steps < TOTAL_TIMESTEPS:\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        max_height = 0\n",
    "        states, actions, rewards, log_probs, values, dones = [], [], [], [], [], []\n",
    "\n",
    "        for t in range(MAX_EPISODE_STEPS):\n",
    "            agent.obs_normalizer.update(np.array([state]))\n",
    "            norm_state = agent.obs_normalizer.normalize(state)\n",
    "            state_tensor = torch.FloatTensor(norm_state).to(device)\n",
    "\n",
    "            action, log_prob, _ = agent.actor.sample(state_tensor.unsqueeze(0))\n",
    "            if action is None:\n",
    "                break\n",
    "            value = agent.critic(state_tensor.unsqueeze(0)).item()\n",
    "\n",
    "            action_np = action.detach().cpu().numpy()[0] * 0.4  # Scale actions to [-0.4, 0.4]\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action_np)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action_np)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob.item())\n",
    "            values.append(value)\n",
    "            dones.append(done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            total_steps += 1\n",
    "            max_height = max(max_height, state[2])  # Track max height\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if np.isnan(next_state).any():\n",
    "            continue\n",
    "\n",
    "        agent.obs_normalizer.update(np.array([next_state]))\n",
    "        norm_next_state = agent.obs_normalizer.normalize(next_state)\n",
    "        next_value = agent.critic(torch.FloatTensor(norm_next_state).to(device).unsqueeze(0)).item()\n",
    "\n",
    "        advantages, returns = agent.compute_gae(rewards, values, next_value, dones)\n",
    "        agent.update(states, actions, log_probs, returns, advantages)\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        avg_reward = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)\n",
    "        print(\n",
    "            f\"Episode: {len(episode_rewards)}, Reward: {episode_reward:.2f}, Avg Reward: {avg_reward:.2f}, Max Height: {max_height:.2f}\")\n",
    "\n",
    "        if len(episode_rewards) >= 10 and avg_reward >= REWARD_THRESHOLD:\n",
    "            threshold_count += 1\n",
    "            if threshold_count >= THRESHOLD_EPISODES:\n",
    "                print(f\"Stopping training: Avg reward {avg_reward:.2f} >= {REWARD_THRESHOLD}\")\n",
    "                break\n",
    "        else:\n",
    "            threshold_count = 0\n",
    "\n",
    "        if len(episode_rewards) >= PLATEAU_WINDOW + 10:\n",
    "            old_avg = np.mean(plateau_rewards[-PLATEAU_WINDOW - 10:-PLATEAU_WINDOW])\n",
    "            new_avg = np.mean(plateau_rewards[-PLATEAU_WINDOW:])\n",
    "            improvement = (new_avg - old_avg) / old_avg if old_avg != 0 else 0\n",
    "            if improvement < PLATEAU_IMPROVEMENT:\n",
    "                print(f\"Stopping training: Reward plateaued (improvement {improvement:.4f} < {PLATEAU_IMPROVEMENT})\")\n",
    "                break\n",
    "            plateau_rewards.append(avg_reward)\n",
    "\n",
    "    agent.save(\"ppo_humanoid_standup_mujoco\")\n",
    "    agent.evaluate(env)\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 31807.09, Avg Reward: 31807.09, Max Height: 0.26\n",
      "Episode: 2, Reward: 32456.98, Avg Reward: 32132.04, Max Height: 0.13\n",
      "Episode: 3, Reward: 36494.93, Avg Reward: 33586.34, Max Height: 0.05\n",
      "Episode: 4, Reward: 32348.89, Avg Reward: 33276.97, Max Height: 0.27\n",
      "Episode: 5, Reward: 33236.40, Avg Reward: 33268.86, Max Height: 0.11\n",
      "Episode: 6, Reward: 31343.53, Avg Reward: 32947.97, Max Height: 0.24\n",
      "Episode: 7, Reward: 32750.48, Avg Reward: 32919.76, Max Height: 0.23\n",
      "Episode: 8, Reward: 33604.78, Avg Reward: 33005.39, Max Height: 0.21\n",
      "Episode: 9, Reward: 34307.34, Avg Reward: 33150.05, Max Height: 0.32\n",
      "Episode: 10, Reward: 36698.17, Avg Reward: 33504.86, Max Height: 0.07\n",
      "Episode: 11, Reward: 32233.83, Avg Reward: 33547.53, Max Height: 0.26\n",
      "Episode: 12, Reward: 31595.73, Avg Reward: 33461.41, Max Height: 0.23\n",
      "Episode: 13, Reward: 33120.82, Avg Reward: 33124.00, Max Height: 0.52\n",
      "Episode: 14, Reward: 30993.87, Avg Reward: 32988.49, Max Height: 0.16\n",
      "Episode: 15, Reward: 36741.01, Avg Reward: 33338.95, Max Height: 0.27\n",
      "Episode: 16, Reward: 33686.58, Avg Reward: 33573.26, Max Height: 0.24\n",
      "Episode: 17, Reward: 37929.63, Avg Reward: 34091.17, Max Height: 0.45\n",
      "Episode: 18, Reward: 32929.50, Avg Reward: 34023.65, Max Height: 0.18\n",
      "Episode: 19, Reward: 34376.56, Avg Reward: 34030.57, Max Height: 0.12\n",
      "Episode: 20, Reward: 31093.65, Avg Reward: 33470.12, Max Height: 0.10\n",
      "Episode: 21, Reward: 31246.19, Avg Reward: 33371.35, Max Height: 0.16\n",
      "Episode: 22, Reward: 31793.47, Avg Reward: 33391.13, Max Height: 0.41\n",
      "Episode: 23, Reward: 34562.83, Avg Reward: 33535.33, Max Height: 0.12\n",
      "Episode: 24, Reward: 30364.90, Avg Reward: 33472.43, Max Height: 0.19\n",
      "Episode: 25, Reward: 31932.96, Avg Reward: 32991.63, Max Height: 0.28\n",
      "Episode: 26, Reward: 34107.66, Avg Reward: 33033.73, Max Height: 0.31\n",
      "Episode: 27, Reward: 32917.76, Avg Reward: 32532.55, Max Height: 0.23\n",
      "Episode: 28, Reward: 32970.58, Avg Reward: 32536.66, Max Height: 0.33\n",
      "Episode: 29, Reward: 34650.27, Avg Reward: 32564.03, Max Height: 0.44\n",
      "Episode: 30, Reward: 32161.25, Avg Reward: 32670.79, Max Height: 0.13\n",
      "Episode: 31, Reward: 32073.34, Avg Reward: 32753.50, Max Height: 0.21\n",
      "Episode: 32, Reward: 33820.92, Avg Reward: 32956.25, Max Height: 0.25\n",
      "Episode: 33, Reward: 30965.23, Avg Reward: 32596.49, Max Height: 0.16\n",
      "Episode: 34, Reward: 34153.03, Avg Reward: 32975.30, Max Height: 0.33\n",
      "Episode: 35, Reward: 38128.81, Avg Reward: 33594.89, Max Height: 0.05\n",
      "Episode: 36, Reward: 34162.61, Avg Reward: 33600.38, Max Height: 0.32\n",
      "Episode: 37, Reward: 33747.13, Avg Reward: 33683.32, Max Height: 0.05\n",
      "Episode: 38, Reward: 37083.64, Avg Reward: 34094.62, Max Height: 0.05\n",
      "Episode: 39, Reward: 33757.92, Avg Reward: 34005.39, Max Height: 0.39\n",
      "Episode: 40, Reward: 33557.11, Avg Reward: 34144.97, Max Height: 0.44\n",
      "Episode: 41, Reward: 35996.42, Avg Reward: 34537.28, Max Height: 0.38\n",
      "Episode: 42, Reward: 35051.16, Avg Reward: 34660.31, Max Height: 0.07\n",
      "Episode: 43, Reward: 30992.97, Avg Reward: 34663.08, Max Height: 0.05\n",
      "Episode: 44, Reward: 35511.62, Avg Reward: 34798.94, Max Height: 0.31\n",
      "Episode: 45, Reward: 30057.80, Avg Reward: 33991.84, Max Height: 0.14\n",
      "Episode: 46, Reward: 33411.02, Avg Reward: 33916.68, Max Height: 0.34\n",
      "Episode: 47, Reward: 37318.03, Avg Reward: 34273.77, Max Height: 0.35\n",
      "Episode: 48, Reward: 30937.04, Avg Reward: 33659.11, Max Height: 0.24\n",
      "Episode: 49, Reward: 34793.91, Avg Reward: 33762.71, Max Height: 0.36\n",
      "Episode: 50, Reward: 31412.73, Avg Reward: 33548.27, Max Height: 0.25\n",
      "Episode: 51, Reward: 32054.93, Avg Reward: 33154.12, Max Height: 0.17\n",
      "Episode: 52, Reward: 32315.96, Avg Reward: 32880.60, Max Height: 0.10\n",
      "Episode: 53, Reward: 33689.19, Avg Reward: 33150.22, Max Height: 0.26\n",
      "Episode: 54, Reward: 30866.83, Avg Reward: 32685.74, Max Height: 0.19\n",
      "Episode: 55, Reward: 33871.93, Avg Reward: 33067.16, Max Height: 0.40\n",
      "Episode: 56, Reward: 35986.38, Avg Reward: 33324.69, Max Height: 0.41\n",
      "Episode: 57, Reward: 35337.28, Avg Reward: 33126.62, Max Height: 0.12\n",
      "Episode: 58, Reward: 31490.57, Avg Reward: 33181.97, Max Height: 0.23\n",
      "Episode: 59, Reward: 34594.41, Avg Reward: 33162.02, Max Height: 0.08\n",
      "Episode: 60, Reward: 38517.06, Avg Reward: 33872.45, Max Height: 0.06\n",
      "Episode: 61, Reward: 33411.58, Avg Reward: 34008.12, Max Height: 0.25\n",
      "Episode: 62, Reward: 36939.52, Avg Reward: 34470.48, Max Height: 0.35\n",
      "Episode: 63, Reward: 34940.18, Avg Reward: 34595.57, Max Height: 0.07\n",
      "Episode: 64, Reward: 33977.32, Avg Reward: 34906.62, Max Height: 0.26\n",
      "Episode: 65, Reward: 32385.24, Avg Reward: 34757.95, Max Height: 0.10\n",
      "Episode: 66, Reward: 34153.82, Avg Reward: 34574.70, Max Height: 0.07\n",
      "Episode: 67, Reward: 32618.13, Avg Reward: 34302.78, Max Height: 0.09\n",
      "Episode: 68, Reward: 33571.35, Avg Reward: 34510.86, Max Height: 0.22\n",
      "Episode: 69, Reward: 33848.39, Avg Reward: 34436.26, Max Height: 0.07\n",
      "Episode: 70, Reward: 30550.30, Avg Reward: 33639.58, Max Height: 0.16\n",
      "Episode: 71, Reward: 35355.82, Avg Reward: 33834.01, Max Height: 0.23\n",
      "Episode: 72, Reward: 35287.37, Avg Reward: 33668.79, Max Height: 0.40\n",
      "Episode: 73, Reward: 34106.58, Avg Reward: 33585.43, Max Height: 0.43\n",
      "Episode: 74, Reward: 32855.63, Avg Reward: 33473.26, Max Height: 0.26\n",
      "Episode: 75, Reward: 34480.54, Avg Reward: 33682.79, Max Height: 0.20\n",
      "Episode: 76, Reward: 30536.29, Avg Reward: 33321.04, Max Height: 0.18\n",
      "Episode: 77, Reward: 39077.47, Avg Reward: 33966.97, Max Height: 0.42\n",
      "Episode: 78, Reward: 36600.46, Avg Reward: 34269.89, Max Height: 0.07\n",
      "Episode: 79, Reward: 32071.96, Avg Reward: 34092.24, Max Height: 0.06\n",
      "Episode: 80, Reward: 32632.51, Avg Reward: 34300.46, Max Height: 0.15\n",
      "Episode: 81, Reward: 34468.69, Avg Reward: 34211.75, Max Height: 0.34\n",
      "Episode: 82, Reward: 42453.24, Avg Reward: 34928.34, Max Height: 0.47\n",
      "Episode: 83, Reward: 32763.73, Avg Reward: 34794.05, Max Height: 0.13\n",
      "Episode: 84, Reward: 31804.53, Avg Reward: 34688.94, Max Height: 0.10\n",
      "Episode: 85, Reward: 32306.82, Avg Reward: 34471.57, Max Height: 0.16\n",
      "Episode: 86, Reward: 31693.91, Avg Reward: 34587.33, Max Height: 0.13\n",
      "Episode: 87, Reward: 36222.29, Avg Reward: 34301.82, Max Height: 0.10\n",
      "Episode: 88, Reward: 30208.57, Avg Reward: 33662.63, Max Height: 0.14\n",
      "Episode: 89, Reward: 32138.54, Avg Reward: 33669.28, Max Height: 0.29\n",
      "Episode: 90, Reward: 40822.76, Avg Reward: 34488.31, Max Height: 0.12\n",
      "Episode: 91, Reward: 31411.88, Avg Reward: 34182.63, Max Height: 0.31\n",
      "Episode: 92, Reward: 32410.46, Avg Reward: 33178.35, Max Height: 0.11\n",
      "Episode: 93, Reward: 33672.31, Avg Reward: 33269.21, Max Height: 0.19\n",
      "Episode: 94, Reward: 38018.13, Avg Reward: 33890.57, Max Height: 0.42\n",
      "Episode: 95, Reward: 30839.18, Avg Reward: 33743.81, Max Height: 0.13\n",
      "Episode: 96, Reward: 31929.74, Avg Reward: 33767.39, Max Height: 0.18\n",
      "Episode: 97, Reward: 34771.13, Avg Reward: 33622.27, Max Height: 0.47\n",
      "Episode: 98, Reward: 30906.70, Avg Reward: 33692.08, Max Height: 0.20\n",
      "Episode: 99, Reward: 32142.04, Avg Reward: 33692.43, Max Height: 0.37\n",
      "Episode: 100, Reward: 36132.84, Avg Reward: 33223.44, Max Height: 0.12\n",
      "Episode: 101, Reward: 38710.95, Avg Reward: 33953.35, Max Height: 0.54\n",
      "Episode: 102, Reward: 32403.78, Avg Reward: 33952.68, Max Height: 0.15\n",
      "Episode: 103, Reward: 34901.72, Avg Reward: 34075.62, Max Height: 0.10\n",
      "Episode: 104, Reward: 40283.98, Avg Reward: 34302.21, Max Height: 0.49\n",
      "Episode: 105, Reward: 30772.70, Avg Reward: 34295.56, Max Height: 0.12\n",
      "Episode: 106, Reward: 31655.05, Avg Reward: 34268.09, Max Height: 0.23\n",
      "Episode: 107, Reward: 30186.02, Avg Reward: 33809.58, Max Height: 0.15\n",
      "Episode: 108, Reward: 33100.24, Avg Reward: 34028.93, Max Height: 0.07\n",
      "Episode: 109, Reward: 35744.68, Avg Reward: 34389.20, Max Height: 0.19\n",
      "Episode: 110, Reward: 30910.59, Avg Reward: 33866.97, Max Height: 0.29\n",
      "Episode: 111, Reward: 31494.69, Avg Reward: 33145.35, Max Height: 0.15\n",
      "Episode: 112, Reward: 32944.02, Avg Reward: 33199.37, Max Height: 0.05\n",
      "Stopping training: Reward plateaued (improvement 0.0033 < 0.005)\n",
      "\n",
      "Starting evaluation...\n",
      "Evaluation Episode 1: Reward: 30510.05, Max Height: 0.18\n",
      "Evaluation Episode 2: Reward: 30380.41, Max Height: 0.21\n",
      "Evaluation Episode 3: Reward: 31784.72, Max Height: 0.29\n",
      "Evaluation Episode 4: Reward: 31990.73, Max Height: 0.07\n",
      "Evaluation Episode 5: Reward: 33045.27, Max Height: 0.14\n",
      "Average Evaluation Reward: 31542.23\n",
      "Standing Success Rate: 0.00 (0/5 episodes)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2b58a18a0dbebc24"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
