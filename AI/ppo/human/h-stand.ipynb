{
 "cells": [
  {
   "cell_type": "code",
   "id": "d5f9b94dabaf04ae",
   "metadata": {},
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import torch\n",
    "\n",
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPSILON = 0.2\n",
    "ACTOR_LR = 3e-4\n",
    "CRITIC_LR = 1e-3  # Not directly used; Stable Baselines3 uses a single learning rate\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 512\n",
    "ENTROPY_COEF = 0.01\n",
    "CONTROL_PENALTY = 0.01\n",
    "MAX_EPISODE_STEPS = 1000\n",
    "ENV_NAME = \"HumanoidStandup-v4\"\n",
    "TOTAL_TIMESTEPS = 10000000\n",
    "REWARD_THRESHOLD = 45000\n",
    "THRESHOLD_EPISODES = 3\n",
    "PLATEAU_WINDOW = 50\n",
    "PLATEAU_IMPROVEMENT = 0.001\n",
    "EVAL_EPISODES = 5\n",
    "EVAL_MAX_STEPS = 2000\n",
    "EVAL_SAVE_VIDEO = True\n",
    "\n",
    "class RunningStat:\n",
    "    def __init__(self, shape):\n",
    "        self.mean = np.zeros(shape, dtype=np.float32)\n",
    "        self.std = np.ones(shape, dtype=np.float32)\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_std = np.std(x, axis=0) + 1e-6\n",
    "        batch_count = x.shape[0]\n",
    "        self.count += batch_count\n",
    "        delta = batch_mean - self.mean\n",
    "        self.mean += delta * batch_count / self.count\n",
    "        m_a = self.std * self.std * (self.count - batch_count)\n",
    "        m_b = batch_std * batch_std * batch_count\n",
    "        self.std = np.sqrt((m_a + m_b + np.square(delta) * self.count * batch_count / self.count) / self.count)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        normalized = (x - self.mean) / self.std\n",
    "        return np.clip(normalized, -5, 5)\n",
    "\n",
    "class NormalizeObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.running_stat = RunningStat(env.observation_space.shape)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        self.running_stat.update(np.array([obs]))\n",
    "        return self.running_stat.normalize(obs)\n",
    "\n",
    "class ControlPenaltyWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, penalty_coeff=CONTROL_PENALTY):\n",
    "        super().__init__(env)\n",
    "        self.penalty_coeff = penalty_coeff\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        reward -= self.penalty_coeff * np.sum(action ** 2)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "def evaluate(model, env, episodes=EVAL_EPISODES, max_steps=EVAL_MAX_STEPS, save_video=EVAL_SAVE_VIDEO):\n",
    "    \"\"\"Evaluate the policy and check if the humanoid stands.\"\"\"\n",
    "    print(\"\\nStarting evaluation...\")\n",
    "    total_reward = 0\n",
    "    standing_episodes = 0\n",
    "    height_threshold = 1.1  # Approximate height for standing (z-position)\n",
    "\n",
    "    if save_video:\n",
    "        video_folder = \"evaluation_videos\"\n",
    "        os.makedirs(video_folder, exist_ok=True)\n",
    "        video_writer = None\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        max_height = 0\n",
    "\n",
    "        if save_video:\n",
    "            video_path = f\"{video_folder}/eval_episode_{ep + 1}.mp4\"\n",
    "            frame = env.render()\n",
    "            if frame is not None:\n",
    "                video_writer = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), 30,\n",
    "                                               (frame.shape[1], frame.shape[0]))\n",
    "            else:\n",
    "                print(\"Warning: Render returned None, skipping video\")\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            action, _ = model.predict(obs, deterministic=False)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            height = obs[2]  # Assuming z-position is at index 2\n",
    "            max_height = max(max_height, height)\n",
    "\n",
    "            if save_video and video_writer is not None:\n",
    "                frame = env.render()\n",
    "                if frame is not None:\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                    video_writer.write(frame)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        if max_height > height_threshold:\n",
    "            standing_episodes += 1\n",
    "\n",
    "        print(f\"Evaluation Episode {ep + 1}: Reward: {episode_reward:.2f}, Max Height: {max_height:.2f}\")\n",
    "        if save_video and video_writer is not None:\n",
    "            video_writer.release()\n",
    "\n",
    "    avg_reward = total_reward / episodes\n",
    "    standing_ratio = standing_episodes / episodes\n",
    "    print(f\"Average Evaluation Reward: {avg_reward:.2f}\")\n",
    "    print(f\"Standing Success Rate: {standing_ratio:.2f} ({standing_episodes}/{episodes} episodes)\")\n",
    "    return avg_reward, standing_ratio\n",
    "\n",
    "def train():\n",
    "    # Create environment with normalization and control penalty\n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\", max_episode_steps=MAX_EPISODE_STEPS)\n",
    "    env = NormalizeObservationWrapper(env)\n",
    "    env = ControlPenaltyWrapper(env, penalty_coeff=CONTROL_PENALTY)\n",
    "    env = DummyVecEnv([lambda: env])  # Wrap in DummyVecEnv for Stable Baselines3\n",
    "\n",
    "    # Initialize PPO model\n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        learning_rate=ACTOR_LR,\n",
    "        n_steps=MAX_EPISODE_STEPS,  # Steps per rollout\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_epochs=EPOCHS,\n",
    "        gamma=GAMMA,\n",
    "        gae_lambda=LAMBDA,\n",
    "        clip_range=CLIP_EPSILON,\n",
    "        ent_coef=ENTROPY_COEF,\n",
    "        verbose=1,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "    total_steps = 0\n",
    "    episode_rewards = []\n",
    "    threshold_count = 0\n",
    "    plateau_rewards = []\n",
    "    episode_count = 0\n",
    "\n",
    "    # Custom training loop to mimic original stopping conditions\n",
    "    while total_steps < TOTAL_TIMESTEPS:\n",
    "        model.learn(total_timesteps=MAX_EPISODE_STEPS, reset_num_timesteps=False)\n",
    "        total_steps += MAX_EPISODE_STEPS\n",
    "        episode_count += 1\n",
    "\n",
    "        # Estimate episode reward (Stable Baselines3 doesn't track this directly)\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not done and steps < MAX_EPISODE_STEPS:\n",
    "            action, _ = model.predict(obs, deterministic=False)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated[0] or truncated[0]\n",
    "            episode_reward += reward[0]\n",
    "            steps += 1\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        avg_reward = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)\n",
    "        print(f\"Episode: {episode_count}, Reward: {episode_reward:.2f}, Avg Reward: {avg_reward:.2f}\")\n",
    "\n",
    "        if len(episode_rewards) >= 10 and avg_reward >= REWARD_THRESHOLD:\n",
    "            threshold_count += 1\n",
    "            if threshold_count >= THRESHOLD_EPISODES:\n",
    "                print(f\"Stopping training: Avg reward {avg_reward:.2f} >= {REWARD_THRESHOLD}\")\n",
    "                break\n",
    "        else:\n",
    "            threshold_count = 0\n",
    "\n",
    "        if len(episode_rewards) >= PLATEAU_WINDOW + 10:\n",
    "            old_avg = np.mean(plateau_rewards[-PLATEAU_WINDOW - 10:-PLATEAU_WINDOW])\n",
    "            new_avg = np.mean(plateau_rewards[-PLATEAU_WINDOW:])\n",
    "            improvement = (new_avg - old_avg) / old_avg if old_avg != 0 else 0\n",
    "            if improvement < PLATEAU_IMPROVEMENT:\n",
    "                print(f\"Stopping training: Reward plateaued (improvement {improvement:.4f} < {PLATEAU_IMPROVEMENT})\")\n",
    "                break\n",
    "            plateau_rewards.append(avg_reward)\n",
    "\n",
    "    # Save model and evaluate once at the end\n",
    "    model.save(\"ppo_humanoid_standup_mujoco\")\n",
    "    evaluate(model, env)\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Create the HumanoidStandup environment\n",
    "env = make_vec_env(\"HumanoidStandup-v4\", n_envs=1)\n",
    "\n",
    "# Initialize the PPO model with a multi-layer perceptron policy\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the model for 1 million timesteps\n",
    "model.learn(total_timesteps=1000000)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"ppo_humanoid_standup_mujoco\")"
   ],
   "id": "e705ab0c5218af15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bcad44491095d7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "aa6911a0dd032084",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b8fd821aeb4ba2e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9ae22d6073650179",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
