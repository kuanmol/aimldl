{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "from gymnasium import register\n",
    "import ale_py\n",
    "try:\n",
    "    import cv2\n",
    "    USE_OPENCV = True\n",
    "except ImportError:\n",
    "    from PIL import Image\n",
    "    USE_OPENCV = False\n",
    "\n",
    "# Register the Atari Tennis environment\n",
    "register(\n",
    "    id='TennisDeterministic-v4',\n",
    "    entry_point='ale_py.env:AtariEnv',\n",
    "    kwargs={'game': 'tennis', 'mode': 0, 'difficulty': 0},\n",
    "    max_episode_steps=10000,\n",
    "    nondeterministic=False,\n",
    ")\n",
    "\n",
    "# === Hyperparameters ===\n",
    "GAMMA = 0.99                # discount factor\n",
    "LAMBDA = 0.9                # GAE parameter\n",
    "CLIP_EPS = 0.2              # PPO clip parameter\n",
    "EPOCHS = 3                  # reduced optimization epochs\n",
    "MINIBATCH_SIZE = 128        # larger minibatch to reduce iterations\n",
    "ACTOR_LR = 1e-4             # actor learning rate\n",
    "CRITIC_LR = 1e-4            # critic learning rate\n",
    "ENTROPY_COEF = 0.05         # entropy coefficient\n",
    "NUM_ENVS = 4                # reduced parallel environments\n",
    "NUM_STEPS = 256             # increased steps to maintain sample size (4*256=1024)\n",
    "NUM_UPDATES = 50000         # total updates\n",
    "SAVE_INTERVAL = 100         # save interval\n",
    "FRAME_STACK = 4             # frames to stack\n",
    "FRAME_SKIP = 2              # frame skip\n",
    "\n",
    "# === Environment Wrapper ===\n",
    "class AtariWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, FRAME_STACK), dtype=np.uint8)\n",
    "        self.action_space = gym.spaces.Discrete(env.action_space.n)\n",
    "        self.frames = deque(maxlen=FRAME_STACK)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, _ = self.env.reset(**kwargs)\n",
    "        obs = self._preprocess(obs)\n",
    "        for _ in range(FRAME_STACK):\n",
    "            self.frames.append(obs)\n",
    "        return np.stack(self.frames, axis=-1), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for _ in range(FRAME_SKIP):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                break\n",
    "        obs = self._preprocess(obs)\n",
    "        self.frames.append(obs)\n",
    "        return np.stack(self.frames, axis=-1), total_reward, done, False, info\n",
    "\n",
    "    def _preprocess(self, frame):\n",
    "        # Convert to grayscale and crop\n",
    "        frame = frame.mean(axis=2).astype(np.uint8)\n",
    "        frame = frame[34:34+160, :160]  # crop\n",
    "        if USE_OPENCV:\n",
    "            frame = cv2.resize(frame, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "        else:\n",
    "            frame = np.array(Image.fromarray(frame).resize((84, 84), Image.BILINEAR))\n",
    "        return frame\n",
    "\n",
    "# === PPO Network Architecture ===\n",
    "class AtariCNN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(FRAME_STACK, 16, kernel_size=8, stride=4),  # reduced filters\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(1568, 256),  # adjusted for smaller feature map\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(1568, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255.0\n",
    "        features = self.features(x)\n",
    "        return self.actor(features), self.critic(features)\n",
    "\n",
    "# === PPO Agent ===\n",
    "class PPO:\n",
    "    def __init__(self, num_actions, device):\n",
    "        self.net = AtariCNN(num_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=ACTOR_LR, eps=1e-5)\n",
    "        self.device = device\n",
    "\n",
    "    def get_action(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = torch.FloatTensor(x).permute(0, 3, 1, 2).to(self.device)\n",
    "            probs, value = self.net(x)\n",
    "            dist = Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy = dist.entropy()\n",
    "        return action.cpu().numpy(), log_prob.cpu().numpy(), value.cpu().numpy(), entropy.cpu().numpy()\n",
    "\n",
    "    def update(self, samples):\n",
    "        obs, actions, old_log_probs, returns, advantages = samples\n",
    "\n",
    "        # Convert to tensors with pinned memory for GPU\n",
    "        obs = torch.FloatTensor(obs).permute(0, 3, 1, 2).to(self.device, non_blocking=True)\n",
    "        actions = torch.LongTensor(actions).to(self.device, non_blocking=True)\n",
    "        old_log_probs = torch.FloatTensor(old_log_probs).to(self.device, non_blocking=True)\n",
    "        returns = torch.FloatTensor(returns).unsqueeze(1).to(self.device, non_blocking=True)\n",
    "        advantages = torch.FloatTensor(advantages).unsqueeze(1).to(self.device, non_blocking=True)\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            indices = torch.randperm(NUM_ENVS * NUM_STEPS)\n",
    "            for start in range(0, len(indices), MINIBATCH_SIZE):\n",
    "                end = start + MINIBATCH_SIZE\n",
    "                idx = indices[start:end]\n",
    "\n",
    "                mb_obs = obs[idx]\n",
    "                mb_actions = actions[idx]\n",
    "                mb_old_log_probs = old_log_probs[idx]\n",
    "                mb_returns = returns[idx]\n",
    "                mb_advantages = advantages[idx]\n",
    "\n",
    "                probs, values = self.net(mb_obs)\n",
    "                dist = Categorical(probs)\n",
    "                log_probs = dist.log_prob(mb_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                ratios = torch.exp(log_probs - mb_old_log_probs)\n",
    "                surr1 = ratios * mb_advantages\n",
    "                surr2 = torch.clamp(ratios, 1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * mb_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean() - ENTROPY_COEF * entropy\n",
    "                critic_loss = 0.5 * (mb_returns - values).pow(2).mean()\n",
    "                loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "                self.optimizer.zero_grad(set_to_none=True)  # faster zero_grad\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.net.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "\n",
    "# === Training Loop ===\n",
    "def train():\n",
    "    # Create environments\n",
    "    envs = gym.vector.SyncVectorEnv([\n",
    "        lambda: AtariWrapper(gym.make(\"TennisDeterministic-v4\", render_mode=\"rgb_array\"))\n",
    "        for _ in range(NUM_ENVS)\n",
    "    ])\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agent = PPO(envs.single_action_space.n, device)\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs(\"\", exist_ok=True)\n",
    "    os.makedirs(\"../videos\", exist_ok=True)\n",
    "\n",
    "    # Training statistics\n",
    "    episode_rewards = np.zeros(NUM_ENVS)\n",
    "    final_episode_rewards = []\n",
    "    best_mean_reward = -np.inf\n",
    "\n",
    "    # Evaluation function\n",
    "    def evaluate(agent, env, num_episodes=5):  # reduced episodes\n",
    "        eval_env = AtariWrapper(gym.make(\"TennisDeterministic-v4\", render_mode=\"rgb_array\"))\n",
    "        rewards = []\n",
    "        for _ in range(num_episodes):\n",
    "            obs, _ = eval_env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action, _, _, _ = agent.get_action(np.expand_dims(obs, 0))\n",
    "                obs, reward, done, _, _ = eval_env.step(action[0])\n",
    "                episode_reward += reward\n",
    "            rewards.append(episode_reward)\n",
    "        eval_env.close()\n",
    "        return np.mean(rewards), np.std(rewards)\n",
    "\n",
    "    # Initialize environments\n",
    "    obs, _ = envs.reset()\n",
    "\n",
    "    for update in range(1, NUM_UPDATES + 1):\n",
    "        # Storage\n",
    "        mb_obs = np.zeros((NUM_STEPS, NUM_ENVS, 84, 84, FRAME_STACK), dtype=np.uint8)\n",
    "        mb_actions = np.zeros((NUM_STEPS, NUM_ENVS), dtype=np.int32)\n",
    "        mb_log_probs = np.zeros((NUM_STEPS, NUM_ENVS), dtype=np.float32)\n",
    "        mb_values = np.zeros((NUM_STEPS, NUM_ENVS), dtype=np.float32)\n",
    "        mb_rewards = np.zeros((NUM_STEPS, NUM_ENVS), dtype=np.float32)\n",
    "        mb_dones = np.zeros((NUM_STEPS, NUM_ENVS), dtype=np.bool_)\n",
    "        mb_entropy = np.zeros((NUM_STEPS, NUM_ENVS), dtype=np.float32)\n",
    "\n",
    "        # Collect rollout\n",
    "        for step in range(NUM_STEPS):\n",
    "            mb_obs[step] = obs\n",
    "            actions, log_probs, values, entropy = agent.get_action(obs)\n",
    "            mb_actions[step] = actions\n",
    "            mb_log_probs[step] = log_probs\n",
    "            mb_values[step] = values.squeeze()\n",
    "            mb_entropy[step] = entropy\n",
    "\n",
    "            step_output = envs.step(actions)\n",
    "            obs, rewards, dones, _, infos = step_output\n",
    "            mb_rewards[step] = rewards\n",
    "            mb_dones[step] = dones\n",
    "\n",
    "            episode_rewards += rewards\n",
    "            for i, done in enumerate(dones):\n",
    "                if done:\n",
    "                    final_episode_rewards.append(episode_rewards[i])\n",
    "                    episode_rewards[i] = 0\n",
    "\n",
    "        # Calculate advantages and returns\n",
    "        with torch.no_grad():\n",
    "            last_values = agent.net(torch.FloatTensor(obs).permute(0, 3, 1, 2).to(device))[1].cpu().numpy()\n",
    "\n",
    "        mb_advantages = np.zeros_like(mb_rewards)\n",
    "        mb_returns = np.zeros_like(mb_rewards)\n",
    "        gae = 0\n",
    "        for t in reversed(range(NUM_STEPS)):\n",
    "            if t == NUM_STEPS - 1:\n",
    "                next_non_terminal = 1.0 - mb_dones[t]\n",
    "                next_values = last_values.squeeze()\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - mb_dones[t+1]\n",
    "                next_values = mb_values[t+1]\n",
    "            delta = mb_rewards[t] + GAMMA * next_values * next_non_terminal - mb_values[t]\n",
    "            gae = delta + GAMMA * LAMBDA * next_non_terminal * gae\n",
    "            mb_advantages[t] = gae\n",
    "            mb_returns[t] = mb_advantages[t] + mb_values[t]\n",
    "\n",
    "        # Flatten the batch\n",
    "        mb_obs = mb_obs.reshape((-1, 84, 84, FRAME_STACK))\n",
    "        mb_actions = mb_actions.flatten()\n",
    "        mb_log_probs = mb_log_probs.flatten()\n",
    "        mb_returns = mb_returns.flatten()\n",
    "        mb_advantages = mb_advantages.flatten()\n",
    "\n",
    "        # Update policy\n",
    "        agent.update((mb_obs, mb_actions, mb_log_probs, mb_returns, mb_advantages))\n",
    "\n",
    "        # Calculate statistics\n",
    "        mean_reward = np.mean(final_episode_rewards[-100:]) if final_episode_rewards else 0\n",
    "\n",
    "        # Print progress\n",
    "        if update % 10 == 0:\n",
    "            print(f\"Update: {update}/{NUM_UPDATES} | \"\n",
    "                  f\"Mean Reward: {mean_reward:.2f} | \"\n",
    "                  f\"Mean Entropy: {np.mean(mb_entropy):.4f} | \"\n",
    "                  f\"Minibatch Size: {len(mb_obs)} | \"\n",
    "                  f\"Total Frames: {update * NUM_STEPS * NUM_ENVS}\")\n",
    "\n",
    "        # Evaluate policy\n",
    "        if update % 100 == 0:\n",
    "            eval_mean, eval_std = evaluate(agent, envs)\n",
    "            print(f\"Eval Mean Reward: {eval_mean:.2f} ± {eval_std:.2f}\")\n",
    "\n",
    "        # Record video (less frequent to save time)\n",
    "        if update % 1000 == 0:  # changed from 500\n",
    "            from gymnasium.wrappers import RecordVideo\n",
    "            eval_env = RecordVideo(\n",
    "                AtariWrapper(gym.make(\"TennisDeterministic-v4\", render_mode=\"rgb_array\")),\n",
    "                video_folder=\"videos\",\n",
    "                name_prefix=f\"tennis_update_{update}\"\n",
    "            )\n",
    "            obs, _ = eval_env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _, _, _ = agent.get_action(np.expand_dims(obs, 0))\n",
    "                obs, _, done, _, _ = eval_env.step(action[0])\n",
    "            eval_env.close()\n",
    "\n",
    "        # Save model\n",
    "        if update % SAVE_INTERVAL == 0:\n",
    "            if mean_reward > best_mean_reward:\n",
    "                best_mean_reward = mean_reward\n",
    "                torch.save(agent.net.state_dict(), \"tennis_ppo_best.pth\")\n",
    "                print(f\"New best model saved with mean reward: {best_mean_reward:.2f}\")\n",
    "            torch.save(agent.net.state_dict(), f\"models/tennis_ppo_{update}.pth\")\n",
    "\n",
    "    # Final save\n",
    "    torch.save(agent.net.state_dict(), \"models/tennis_ppo_final.pth\")\n",
    "    envs.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Enable mixed precision if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.amp.autocast(enabled=True)\n",
    "    train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import time\n",
    "from gymnasium import register\n",
    "import ale_py\n",
    "from PIL import Image\n",
    "\n",
    "# Register the Atari Tennis environment\n",
    "register(\n",
    "    id='TennisDeterministic-v4',\n",
    "    entry_point='ale_py.env:AtariEnv',\n",
    "    kwargs={'game': 'tennis', 'mode': 0, 'difficulty': 0},\n",
    "    max_episode_steps=10000,\n",
    "    nondeterministic=False,\n",
    ")\n",
    "\n",
    "# === Hyperparameters ===\n",
    "GAMMA = 0.99                # discount factor\n",
    "LAMBDA = 0.95               # GAE parameter\n",
    "CLIP_EPS = 0.1              # PPO clip parameter\n",
    "EPOCHS = 4                  # number of optimization epochs per update\n",
    "MINIBATCH_SIZE = 64         # mini-batch size\n",
    "ACTOR_LR = 2.5e-4           # actor learning rate\n",
    "CRITIC_LR = 2.5e-4          # critic learning rate\n",
    "ENTROPY_COEF = 0.01         # entropy coefficient\n",
    "NUM_ENVS = 8                # number of parallel environments\n",
    "NUM_STEPS = 128             # steps per environment per update\n",
    "NUM_UPDATES = 10000         # total number of updates\n",
    "SAVE_INTERVAL = 100         # save interval (updates)\n",
    "FRAME_STACK = 4             # number of frames to stack\n",
    "FRAME_SKIP = 4              # frame skip (action repeat)\n",
    "\n",
    "# === Environment Wrapper ===\n",
    "class AtariWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, FRAME_STACK), dtype=np.uint8)\n",
    "        self.action_space = gym.spaces.Discrete(env.action_space.n)\n",
    "        self.frames = deque(maxlen=FRAME_STACK)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, _ = self.env.reset(**kwargs)\n",
    "        obs = self._preprocess(obs)\n",
    "        for _ in range(FRAME_STACK):\n",
    "            self.frames.append(obs)\n",
    "        return np.stack(self.frames, axis=-1), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for _ in range(FRAME_SKIP):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        obs = self._preprocess(obs)\n",
    "        self.frames.append(obs)\n",
    "        stacked_frames = np.stack(self.frames, axis=-1)\n",
    "        return stacked_frames, total_reward, done, False, info\n",
    "\n",
    "    def _preprocess(self, frame):\n",
    "        # Convert to grayscale and resize\n",
    "        frame = np.array(Image.fromarray(frame).convert('L').resize((84, 84), Image.BILINEAR))\n",
    "        return frame\n",
    "\n",
    "# === PPO Network Architecture ===\n",
    "class AtariCNN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(FRAME_STACK, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255.0\n",
    "        features = self.features(x)\n",
    "        return self.actor(features), self.critic(features)\n",
    "\n",
    "# === PPO Agent ===\n",
    "class PPO:\n",
    "    def __init__(self, num_actions, device):\n",
    "        self.net = AtariCNN(num_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=ACTOR_LR, eps=1e-5)\n",
    "        self.device = device\n",
    "\n",
    "    def get_action(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = torch.FloatTensor(x).permute(0, 3, 1, 2).to(self.device)\n",
    "            probs, value = self.net(x)\n",
    "            dist = Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy = dist.entropy()\n",
    "        return action.cpu().numpy(), log_prob.cpu().numpy(), value.cpu().numpy(), entropy.cpu().numpy()\n",
    "\n",
    "    def update(self, samples):\n",
    "        obs, actions, old_log_probs, returns, advantages = samples\n",
    "\n",
    "        # Convert to tensors\n",
    "        obs = torch.FloatTensor(np.asarray(obs)).permute(0, 3, 1, 2).to(self.device)\n",
    "        actions = torch.LongTensor(np.asarray(actions)).to(self.device)\n",
    "        old_log_probs = torch.FloatTensor(np.asarray(old_log_probs)).to(self.device)\n",
    "        returns = torch.FloatTensor(np.asarray(returns)).unsqueeze(1).to(self.device)\n",
    "        advantages = torch.FloatTensor(np.asarray(advantages)).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            # Shuffle indices\n",
    "            indices = torch.randperm(len(obs))\n",
    "\n",
    "            for start in range(0, len(indices), MINIBATCH_SIZE):\n",
    "                end = start + MINIBATCH_SIZE\n",
    "                idx = indices[start:end]\n",
    "\n",
    "                # Get minibatch\n",
    "                mb_obs = obs[idx]\n",
    "                mb_actions = actions[idx]\n",
    "                mb_old_log_probs = old_log_probs[idx]\n",
    "                mb_returns = returns[idx]\n",
    "                mb_advantages = advantages[idx]\n",
    "\n",
    "                # Get current policy\n",
    "                probs, values = self.net(mb_obs)\n",
    "                dist = Categorical(probs)\n",
    "                log_probs = dist.log_prob(mb_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                # Calculate ratios\n",
    "                ratios = torch.exp(log_probs - mb_old_log_probs)\n",
    "\n",
    "                # Actor loss\n",
    "                surr1 = ratios * mb_advantages\n",
    "                surr2 = torch.clamp(ratios, 1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * mb_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean() - ENTROPY_COEF * entropy\n",
    "\n",
    "                # Critic loss\n",
    "                critic_loss = 0.5 * (mb_returns - values).pow(2).mean()\n",
    "\n",
    "                # Total loss\n",
    "                loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "                # Update\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.net.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "\n",
    "# === Training Loop ===\n",
    "def train():\n",
    "    # Create environments\n",
    "    envs = gym.vector.SyncVectorEnv([\n",
    "        lambda: AtariWrapper(gym.make(\"TennisDeterministic-v4\", render_mode=\"rgb_array\"))\n",
    "        for _ in range(NUM_ENVS)\n",
    "    ])\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agent = PPO(envs.single_action_space.n, device)\n",
    "\n",
    "    # Create directory for saving models\n",
    "    os.makedirs(\"\", exist_ok=True)\n",
    "\n",
    "    # Training statistics\n",
    "    episode_rewards = np.zeros(NUM_ENVS)\n",
    "    final_episode_rewards = []\n",
    "    best_mean_reward = -np.inf\n",
    "\n",
    "    # Initialize environments\n",
    "    obs, _ = envs.reset()\n",
    "\n",
    "    for update in range(1, NUM_UPDATES + 1):\n",
    "        # Storage\n",
    "        mb_obs = np.zeros((NUM_STEPS, NUM_ENVS, 84, 84, FRAME_STACK), dtype=np.uint8)\n",
    "        mb_actions = np.zeros((NUM_STEPS, NUM_ENVS), dtype=np.int32)\n",
    "        mb_log_probs = np.zeros((NUM_STEPS, NUM_ENVS), dtype=np.float32)\n",
    "        mb_values = np.zeros((NUM_STEPS, NUM_ENVS), dtype=np.float32)\n",
    "        mb_rewards = np.zeros((NUM_STEPS, NUM_ENVS), dtype=np.float32)\n",
    "        mb_dones = np.zeros((NUM_STEPS, NUM_ENVS), dtype=np.bool_)\n",
    "        mb_entropy = np.zeros((NUM_STEPS, NUM_ENVS), dtype=np.float32)\n",
    "\n",
    "        # Collect rollout\n",
    "        for step in range(NUM_STEPS):\n",
    "            mb_obs[step] = obs\n",
    "            actions, log_probs, values, entropy = agent.get_action(obs)\n",
    "            mb_actions[step] = actions\n",
    "            mb_log_probs[step] = log_probs\n",
    "            mb_values[step] = values.squeeze()\n",
    "            mb_entropy[step] = entropy\n",
    "\n",
    "            obs, rewards, dones, _, infos = envs.step(actions)\n",
    "            mb_rewards[step] = rewards\n",
    "            mb_dones[step] = dones\n",
    "\n",
    "            # Track episode rewards\n",
    "            episode_rewards += rewards\n",
    "            for i, done in enumerate(dones):\n",
    "                if done:\n",
    "                    final_episode_rewards.append(episode_rewards[i])\n",
    "                    episode_rewards[i] = 0\n",
    "\n",
    "        # Calculate advantages and returns\n",
    "        with torch.no_grad():\n",
    "            last_values = agent.net(torch.FloatTensor(obs).permute(0, 3, 1, 2).to(device))[1].cpu().numpy()\n",
    "\n",
    "        mb_advantages = np.zeros_like(mb_rewards)\n",
    "        mb_returns = np.zeros_like(mb_rewards)\n",
    "        gae = 0\n",
    "\n",
    "        for t in reversed(range(NUM_STEPS)):\n",
    "            if t == NUM_STEPS - 1:\n",
    "                next_non_terminal = 1.0 - mb_dones[t]\n",
    "                next_values = last_values.squeeze()\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - mb_dones[t+1]\n",
    "                next_values = mb_values[t+1]\n",
    "\n",
    "            delta = mb_rewards[t] + GAMMA * next_values * next_non_terminal - mb_values[t]\n",
    "            gae = delta + GAMMA * LAMBDA * next_non_terminal * gae\n",
    "            mb_advantages[t] = gae\n",
    "            mb_returns[t] = mb_advantages[t] + mb_values[t]\n",
    "\n",
    "        # Flatten the batch\n",
    "        mb_obs = mb_obs.reshape((-1, 84, 84, FRAME_STACK))\n",
    "        mb_actions = mb_actions.flatten()\n",
    "        mb_log_probs = mb_log_probs.flatten()\n",
    "        mb_returns = mb_returns.flatten()\n",
    "        mb_advantages = mb_advantages.flatten()\n",
    "\n",
    "        # Update policy\n",
    "        agent.update((mb_obs, mb_actions, mb_log_probs, mb_returns, mb_advantages))\n",
    "\n",
    "        # Calculate statistics\n",
    "        mean_reward = np.mean(final_episode_rewards[-100:]) if final_episode_rewards else 0\n",
    "\n",
    "        # Print progress\n",
    "        if update % 10 == 0:\n",
    "            print(f\"Update: {update}/{NUM_UPDATES} | \"\n",
    "                  f\"Mean Reward: {mean_reward:.2f} | \"\n",
    "                  f\"Minibatch Size: {len(mb_obs)} | \"\n",
    "                  f\"Total Frames: {update * NUM_STEPS * NUM_ENVS}\")\n",
    "\n",
    "        # Save model\n",
    "        if update % SAVE_INTERVAL == 0:\n",
    "            if mean_reward > best_mean_reward:\n",
    "                best_mean_reward = mean_reward\n",
    "                torch.save(agent.net.state_dict(), \"tennis_ppo_best.pth\")\n",
    "                print(f\"New best model saved with mean reward: {best_mean_reward:.2f}\")\n",
    "\n",
    "            torch.save(agent.net.state_dict(), f\"models/tennis_ppo_{update}.pth\")\n",
    "\n",
    "    # Final save\n",
    "    torch.save(agent.net.state_dict(), \"models/tennis_ppo_final.pth\")\n",
    "    envs.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ],
   "id": "bf2e1bed5b0afe49",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
