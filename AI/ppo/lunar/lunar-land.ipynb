{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-21T08:57:27.479281Z",
     "start_time": "2025-04-21T08:57:27.024534Z"
    }
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import cloudpickle\n",
    "\n",
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPSILON = 0.2\n",
    "ACTOR_LR = 3e-4\n",
    "CRITIC_LR = 1e-3\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "ENTROPY_COEF = 0.01\n",
    "GRAD_CLIP = 0.5\n",
    "MAX_EPISODE_STEPS = 1000\n",
    "ENV_NAME = \"LunarLander-v3\"\n",
    "TOTAL_TIMESTEPS = 1000000\n",
    "TARGET_REWARD = 200\n",
    "EVAL_EPISODES = 10\n",
    "EVAL_MAX_STEPS = 1000\n",
    "EVAL_SAVE_VIDEO = True\n",
    "SOLVED_EPISODES = 100\n",
    "\n",
    "class RunningStat:\n",
    "    def __init__(self, shape):\n",
    "        self.mean = np.zeros(shape, dtype=np.float32)\n",
    "        self.std = np.ones(shape, dtype=np.float32)\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_std = np.std(x, axis=0) + 1e-6\n",
    "        batch_count = x.shape[0]\n",
    "        self.count += batch_count\n",
    "        delta = batch_mean - self.mean\n",
    "        self.mean += delta * batch_count / self.count\n",
    "        m_a = self.std * self.std * (self.count - batch_count)\n",
    "        m_b = batch_std * batch_std * batch_count\n",
    "        self.std = np.sqrt((m_a + m_b + np.square(delta) * self.count * batch_count / self.count) / self.count)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        normalized = (x - self.mean) / self.std\n",
    "        return np.clip(normalized, -5, 5)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, act_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        nn.init.orthogonal_(self.net[-2].weight, gain=0.01)\n",
    "        nn.init.zeros_(self.net[-2].bias)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "    def sample(self, state):\n",
    "        probs = self.forward(state)\n",
    "        if torch.isnan(probs).any():\n",
    "            return None, None, None\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return action, log_prob, entropy\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        nn.init.orthogonal_(self.net[-1].weight, gain=1.0)\n",
    "        nn.init.zeros_(self.net[-1].bias)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, obs_dim, act_dim, device):\n",
    "        self.actor = Actor(obs_dim, act_dim).to(device)\n",
    "        self.critic = Critic(obs_dim).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=ACTOR_LR)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=CRITIC_LR)\n",
    "        self.device = device\n",
    "        self.obs_normalizer = RunningStat(obs_dim)\n",
    "\n",
    "    def compute_gae(self, rewards, values, next_value, dones):\n",
    "        advantages = np.zeros_like(rewards)\n",
    "        gae = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + GAMMA * next_value * (1 - dones[t]) - values[t]\n",
    "            gae = delta + GAMMA * LAMBDA * (1 - dones[t]) * gae\n",
    "            advantages[t] = gae\n",
    "            next_value = values[t]\n",
    "        returns = advantages + values\n",
    "        return advantages, returns\n",
    "\n",
    "    def update(self, states, actions, old_log_probs, returns, advantages):\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        old_log_probs = torch.FloatTensor(old_log_probs).to(self.device)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages = torch.FloatTensor(advantages).to(self.device)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            indices = np.random.permutation(len(states))\n",
    "            for start in range(0, len(states), BATCH_SIZE):\n",
    "                batch_idx = indices[start:start + BATCH_SIZE]\n",
    "                batch_states = states[batch_idx]\n",
    "                batch_actions = actions[batch_idx]\n",
    "                batch_old_log_probs = old_log_probs[batch_idx]\n",
    "                batch_returns = returns[batch_idx]\n",
    "                batch_advantages = advantages[batch_idx]\n",
    "\n",
    "                action_out, log_prob, entropy = self.actor.sample(batch_states)\n",
    "                if action_out is None:\n",
    "                    continue\n",
    "                ratio = torch.exp(log_prob - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - CLIP_EPSILON, 1 + CLIP_EPSILON) * batch_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean() - ENTROPY_COEF * entropy.mean()\n",
    "\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), GRAD_CLIP)\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                value = self.critic(batch_states).squeeze()\n",
    "                critic_loss = (batch_returns - value).pow(2).mean()\n",
    "\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), GRAD_CLIP)\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, 'wb') as f:\n",
    "            cloudpickle.dump(self, f)\n",
    "\n",
    "    def evaluate(self, env, episodes=EVAL_EPISODES, max_steps=EVAL_MAX_STEPS, save_video=EVAL_SAVE_VIDEO):\n",
    "        print(\"\\nStarting evaluation...\")\n",
    "        total_reward = 0\n",
    "        successful_landings = 0\n",
    "        reward_threshold = 200\n",
    "\n",
    "        if save_video:\n",
    "            video_folder = \"evaluation_videos\"\n",
    "            os.makedirs(video_folder, exist_ok=True)\n",
    "            video_writer = None\n",
    "\n",
    "        for ep in range(episodes):\n",
    "            obs, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            done = False\n",
    "\n",
    "            if save_video:\n",
    "                video_path = f\"{video_folder}/eval_episode_{ep + 1}.mp4\"\n",
    "                frame = env.render()\n",
    "                if frame is not None:\n",
    "                    video_writer = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), 30,\n",
    "                                                   (frame.shape[1], frame.shape[0]))\n",
    "                else:\n",
    "                    print(\"Warning: Render returned None, skipping video\")\n",
    "\n",
    "            while not done and steps < max_steps:\n",
    "                norm_obs = self.obs_normalizer.normalize(obs)\n",
    "                obs_tensor = torch.FloatTensor(norm_obs).to(self.device).unsqueeze(0)\n",
    "                probs = self.actor(obs_tensor)\n",
    "                dist = Categorical(probs)\n",
    "                action = dist.sample()\n",
    "                if action is None:\n",
    "                    print(\"Warning: Invalid action, breaking episode\")\n",
    "                    break\n",
    "                action_np = action.detach().cpu().numpy().item()\n",
    "\n",
    "                obs, reward, terminated, truncated, info = env.step(action_np)\n",
    "                done = terminated or truncated\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "                if save_video and video_writer is not None:\n",
    "                    frame = env.render()\n",
    "                    if frame is not None:\n",
    "                        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                        video_writer.write(frame)\n",
    "\n",
    "            total_reward += episode_reward\n",
    "            if episode_reward >= reward_threshold:\n",
    "                successful_landings += 1\n",
    "\n",
    "            print(f\"Evaluation Episode {ep + 1}: Reward: {episode_reward:.2f}\")\n",
    "            if save_video and video_writer is not None:\n",
    "                video_writer.release()\n",
    "\n",
    "        avg_reward = total_reward / episodes\n",
    "        success_rate = successful_landings / episodes\n",
    "        print(f\"Average Evaluation Reward: {avg_reward:.2f}\")\n",
    "        print(f\"Success Rate (>= {reward_threshold}): {success_rate:.2f} ({successful_landings}/{episodes} episodes)\")\n",
    "        return avg_reward, success_rate\n",
    "\n",
    "def train():\n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.n\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agent = PPO(obs_dim, act_dim, device)\n",
    "\n",
    "    total_steps = 0\n",
    "    episode_rewards = []\n",
    "    solved_count = 0\n",
    "\n",
    "    while total_steps < TOTAL_TIMESTEPS:\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        states, actions, rewards, log_probs, values, dones = [], [], [], [], [], []\n",
    "\n",
    "        for t in range(MAX_EPISODE_STEPS):\n",
    "            agent.obs_normalizer.update(np.array([state]))\n",
    "            norm_state = agent.obs_normalizer.normalize(state)\n",
    "            state_tensor = torch.FloatTensor(norm_state).to(device)\n",
    "\n",
    "            action, log_prob, _ = agent.actor.sample(state_tensor.unsqueeze(0))\n",
    "            if action is None:\n",
    "                break\n",
    "            value = agent.critic(state_tensor.unsqueeze(0)).item()\n",
    "\n",
    "            action_np = action.detach().cpu().numpy().item()\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action_np)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action_np)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob.item())\n",
    "            values.append(value)\n",
    "            dones.append(done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            total_steps += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if np.isnan(next_state).any():\n",
    "            continue\n",
    "\n",
    "        agent.obs_normalizer.update(np.array([next_state]))\n",
    "        norm_next_state = agent.obs_normalizer.normalize(next_state)\n",
    "        next_value = agent.critic(torch.FloatTensor(norm_next_state).to(device).unsqueeze(0)).item()\n",
    "\n",
    "        advantages, returns = agent.compute_gae(rewards, values, next_value, dones)\n",
    "        agent.update(states, actions, log_probs, returns, advantages)\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        avg_reward = np.mean(episode_rewards[-SOLVED_EPISODES:]) if len(episode_rewards) >= SOLVED_EPISODES else np.mean(episode_rewards)\n",
    "        print(f\"Episode: {len(episode_rewards)}, Reward: {episode_reward:.2f}, Avg Reward: {avg_reward:.2f}\")\n",
    "\n",
    "        if len(episode_rewards) >= SOLVED_EPISODES and avg_reward >= TARGET_REWARD:\n",
    "            solved_count += 1\n",
    "            if solved_count >= 3:\n",
    "                print(f\"Stopping training: Avg reward {avg_reward:.2f} >= {TARGET_REWARD} for 3 consecutive checks\")\n",
    "                break\n",
    "        else:\n",
    "            solved_count = 0\n",
    "\n",
    "    agent.save(\"ppo_lunar_lander\")\n",
    "    agent.evaluate(env)\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ],
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Box2D is not installed, you can install it by run `pip install swig` followed by `pip install \"gymnasium[box2d]\"`",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Artificial\\.venv\\Lib\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:15\u001B[39m\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mBox2D\u001B[39;00m\n\u001B[32m     16\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mBox2D\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mb2\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     17\u001B[39m         circleShape,\n\u001B[32m     18\u001B[39m         contactListener,\n\u001B[32m   (...)\u001B[39m\u001B[32m     22\u001B[39m         revoluteJointDef,\n\u001B[32m     23\u001B[39m     )\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'Box2D'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mDependencyNotInstalled\u001B[39m                    Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 290\u001B[39m\n\u001B[32m    287\u001B[39m     env.close()\n\u001B[32m    289\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m290\u001B[39m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 220\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    219\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtrain\u001B[39m():\n\u001B[32m--> \u001B[39m\u001B[32m220\u001B[39m     env = \u001B[43mgym\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmake\u001B[49m\u001B[43m(\u001B[49m\u001B[43mENV_NAME\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrender_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrgb_array\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    221\u001B[39m     obs_dim = env.observation_space.shape[\u001B[32m0\u001B[39m]\n\u001B[32m    222\u001B[39m     act_dim = env.action_space.n\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Artificial\\.venv\\Lib\\site-packages\\gymnasium\\envs\\registration.py:704\u001B[39m, in \u001B[36mmake\u001B[39m\u001B[34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001B[39m\n\u001B[32m    701\u001B[39m     env_creator = env_spec.entry_point\n\u001B[32m    702\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    703\u001B[39m     \u001B[38;5;66;03m# Assume it's a string\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m704\u001B[39m     env_creator = \u001B[43mload_env_creator\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv_spec\u001B[49m\u001B[43m.\u001B[49m\u001B[43mentry_point\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    706\u001B[39m \u001B[38;5;66;03m# Determine if to use the rendering\u001B[39;00m\n\u001B[32m    707\u001B[39m render_modes: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Artificial\\.venv\\Lib\\site-packages\\gymnasium\\envs\\registration.py:551\u001B[39m, in \u001B[36mload_env_creator\u001B[39m\u001B[34m(name)\u001B[39m\n\u001B[32m    542\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001B[39;00m\n\u001B[32m    543\u001B[39m \n\u001B[32m    544\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    548\u001B[39m \u001B[33;03m    The environment constructor for the given environment name.\u001B[39;00m\n\u001B[32m    549\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    550\u001B[39m mod_name, attr_name = name.split(\u001B[33m\"\u001B[39m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m551\u001B[39m mod = \u001B[43mimportlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    552\u001B[39m fn = \u001B[38;5;28mgetattr\u001B[39m(mod, attr_name)\n\u001B[32m    553\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m fn\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:90\u001B[39m, in \u001B[36mimport_module\u001B[39m\u001B[34m(name, package)\u001B[39m\n\u001B[32m     88\u001B[39m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m     89\u001B[39m         level += \u001B[32m1\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1387\u001B[39m, in \u001B[36m_gcd_import\u001B[39m\u001B[34m(name, package, level)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1360\u001B[39m, in \u001B[36m_find_and_load\u001B[39m\u001B[34m(name, import_)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1310\u001B[39m, in \u001B[36m_find_and_load_unlocked\u001B[39m\u001B[34m(name, import_)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:488\u001B[39m, in \u001B[36m_call_with_frames_removed\u001B[39m\u001B[34m(f, *args, **kwds)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1387\u001B[39m, in \u001B[36m_gcd_import\u001B[39m\u001B[34m(name, package, level)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1360\u001B[39m, in \u001B[36m_find_and_load\u001B[39m\u001B[34m(name, import_)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1331\u001B[39m, in \u001B[36m_find_and_load_unlocked\u001B[39m\u001B[34m(name, import_)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:935\u001B[39m, in \u001B[36m_load_unlocked\u001B[39m\u001B[34m(spec)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap_external>:999\u001B[39m, in \u001B[36mexec_module\u001B[39m\u001B[34m(self, module)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:488\u001B[39m, in \u001B[36m_call_with_frames_removed\u001B[39m\u001B[34m(f, *args, **kwds)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Artificial\\.venv\\Lib\\site-packages\\gymnasium\\envs\\box2d\\__init__.py:1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgymnasium\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01menvs\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbox2d\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbipedal_walker\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgymnasium\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01menvs\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbox2d\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcar_racing\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CarRacing\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgymnasium\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01menvs\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbox2d\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mlunar_lander\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LunarLander, LunarLanderContinuous\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Artificial\\.venv\\Lib\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:25\u001B[39m\n\u001B[32m     16\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mBox2D\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mb2\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     17\u001B[39m         circleShape,\n\u001B[32m     18\u001B[39m         contactListener,\n\u001B[32m   (...)\u001B[39m\u001B[32m     22\u001B[39m         revoluteJointDef,\n\u001B[32m     23\u001B[39m     )\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m DependencyNotInstalled(\n\u001B[32m     26\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mBox2D is not installed, you can install it by run `pip install swig` followed by `pip install \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mgymnasium[box2d]\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m`\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m     27\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n\u001B[32m     31\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpygame\u001B[39;00m\n",
      "\u001B[31mDependencyNotInstalled\u001B[39m: Box2D is not installed, you can install it by run `pip install swig` followed by `pip install \"gymnasium[box2d]\"`"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
