{
 "cells": [
  {
   "cell_type": "code",
   "id": "45300318af4d88f4",
   "metadata": {},
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "ACTOR_LR = 5e-4\n",
    "CRITIC_LR = 5e-4\n",
    "HIDDEN = 256\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, act_dim), nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        self.actor = Actor(obs_dim, act_dim)\n",
    "        self.critic = Critic(obs_dim)\n",
    "        self.opt_actor = optim.Adam(self.actor.parameters(), lr=ACTOR_LR)\n",
    "        self.opt_critic = optim.Adam(self.critic.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "    def compute_adv(self, rewards, values, dones):\n",
    "        adv, ret, gae, next_val = [], [], 0, 0\n",
    "        for r, v, d in zip(reversed(rewards), reversed(values), reversed(dones)):\n",
    "            delta = r + GAMMA * next_val * (1 - d) - v\n",
    "            gae = delta + GAMMA * LAMBDA * gae * (1 - d)\n",
    "            adv.insert(0, gae)\n",
    "            ret.insert(0, gae + v)\n",
    "            next_val = v\n",
    "        return torch.tensor(adv, dtype=torch.float32), torch.tensor(ret, dtype=torch.float32)\n",
    "\n",
    "    def update(self, states, actions, old_logps, advs, rets):\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        actions = torch.tensor(actions)\n",
    "        old_logps = torch.tensor(old_logps)\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            for i in range(0, len(states), BATCH_SIZE):\n",
    "                idx = slice(i, i + BATCH_SIZE)\n",
    "                logits = self.actor(states[idx])\n",
    "                dist = Categorical(logits)\n",
    "                logps = dist.log_prob(actions[idx])\n",
    "                ratio = torch.exp(logps - old_logps[idx])\n",
    "                s1 = ratio * advs[idx]\n",
    "                s2 = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS) * advs[idx]\n",
    "                loss_actor = -torch.min(s1, s2).mean()\n",
    "\n",
    "                vals = self.critic(states[idx]).squeeze()\n",
    "                loss_critic = nn.MSELoss()(vals, rets[idx])\n",
    "\n",
    "                self.opt_actor.zero_grad()\n",
    "                loss_actor.backward()\n",
    "                self.opt_actor.step()\n",
    "\n",
    "                self.opt_critic.zero_grad()\n",
    "                loss_critic.backward()\n",
    "                self.opt_critic.step()\n",
    "\n",
    "def normalize_state(state):\n",
    "    # Normalize position (-1.2 to 0.6) and velocity (-0.07 to 0.07) to [-1, 1]\n",
    "    position, velocity = state\n",
    "    norm_position = (position - (-1.2)) / (0.6 - (-1.2)) * 2 - 1\n",
    "    norm_velocity = (velocity - (-0.07)) / (0.07 - (-0.07)) * 2 - 1\n",
    "    return np.array([norm_position, norm_velocity])\n",
    "\n",
    "def train():\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    agent = PPO(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "    number_episodes = 2000\n",
    "    max_timesteps = 1000\n",
    "    scores_on_100_episodes = deque(maxlen=100)\n",
    "    best_avg_score = -np.inf\n",
    "\n",
    "    for ep in range(number_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = normalize_state(state)\n",
    "        done = False\n",
    "        logps, vals, rewards, states, actions, dones = [], [], [], [], [], []\n",
    "        total = 0  # Tracks base reward for stopping condition\n",
    "\n",
    "        for t in range(max_timesteps):\n",
    "            action, logp = agent.get_action(state)\n",
    "            value = agent.critic(torch.tensor(state, dtype=torch.float32).unsqueeze(0)).item()\n",
    "            next_state, base_reward, done, _, _ = env.step(action)\n",
    "            next_state = normalize_state(next_state)\n",
    "\n",
    "            # Reward shaping for policy learning\n",
    "            shaped_reward = base_reward + 5 * abs(next_state[1])  # Reduced bonus\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(shaped_reward)  # Use shaped reward for learning\n",
    "            logps.append(logp)\n",
    "            vals.append(value)\n",
    "            dones.append(done)\n",
    "\n",
    "            state = next_state\n",
    "            total += base_reward  # Accumulate base reward for scoring\n",
    "\n",
    "            if done or t == max_timesteps - 1:\n",
    "                break\n",
    "\n",
    "        advs, rets = agent.compute_adv(rewards, vals, dones)\n",
    "        agent.update(states, actions, logps, advs, rets)\n",
    "\n",
    "        scores_on_100_episodes.append(total)\n",
    "        avg_score = np.mean(scores_on_100_episodes) if len(scores_on_100_episodes) > 0 else total\n",
    "\n",
    "        # Print progress\n",
    "        print(f'\\rEpisode {ep}\\tAverage Score: {avg_score:.2f}', end=\"\")\n",
    "        if ep % 100 == 0:\n",
    "            print(f'\\rEpisode {ep}\\tAverage Score: {avg_score:.2f}')\n",
    "\n",
    "        # Save best model (based on base reward)\n",
    "        if len(scores_on_100_episodes) == 100 and avg_score > best_avg_score:\n",
    "            best_avg_score = avg_score\n",
    "            torch.save({\n",
    "                'actor_state_dict': agent.actor.state_dict(),\n",
    "                'critic_state_dict': agent.critic.state_dict(),\n",
    "            }, 'ppo_mountaincar_v0_best.pth')\n",
    "\n",
    "        # Check if solved (based on base reward)\n",
    "        if len(scores_on_100_episodes) == 100 and avg_score >= -110:\n",
    "            print(f'\\nEnvironment solved in {ep - 100} episodes!\\tAverage Score: {avg_score:.2f}')\n",
    "            torch.save({\n",
    "                'actor_state_dict': agent.actor.state_dict(),\n",
    "                'critic_state_dict': agent.critic.state_dict(),\n",
    "            }, 'ppo_mountaincar_v0_solved.pth')\n",
    "            break\n",
    "\n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'actor_state_dict': agent.actor.state_dict(),\n",
    "        'critic_state_dict': agent.critic.state_dict(),\n",
    "    }, 'ppo_mountaincar_v0_final.pth')\n",
    "\n",
    "    env.close()\n",
    "    return agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    agent = train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": "",
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
