{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-25T13:34:34.041978Z",
     "start_time": "2025-04-25T13:28:03.086941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 258\n",
    "ACTOR_LR = 1e-4\n",
    "CRITIC_LR = 1e-4\n",
    "HIDDEN = 256\n",
    "ENTROPY_COEF = 0.02\n",
    "NUM_EPISODES = 2000\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "\n",
    "# === Actor & Critic Networks ===\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, act_dim), nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# === PPO Agent ===\n",
    "class PPO:\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        self.actor = Actor(obs_dim, act_dim)\n",
    "        self.critic = Critic(obs_dim)\n",
    "        self.opt_actor = optim.Adam(self.actor.parameters(), lr=ACTOR_LR)\n",
    "        self.opt_critic = optim.Adam(self.critic.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.as_tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        a = dist.sample()\n",
    "        return a.item(), dist.log_prob(a).detach(), dist.entropy().detach()\n",
    "\n",
    "    def compute_adv(self, rewards, values, dones):\n",
    "        advs, rets = [], []\n",
    "        gae, next_val = 0.0, 0.0\n",
    "        for r, v, d in zip(reversed(rewards), reversed(values), reversed(dones)):\n",
    "            delta = r + GAMMA * next_val * (1 - d) - v\n",
    "            gae = delta + GAMMA * LAMBDA * gae * (1 - d)\n",
    "            advs.insert(0, gae)\n",
    "            rets.insert(0, gae + v)\n",
    "            next_val = v\n",
    "        advs = torch.tensor(advs, dtype=torch.float32)\n",
    "        rets = torch.tensor(rets, dtype=torch.float32)\n",
    "        return advs, rets\n",
    "\n",
    "    def update(self, states, actions, old_logps, advs, rets):\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        old_logps = torch.tensor(old_logps, dtype=torch.float32)\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            for start in range(0, len(states), BATCH_SIZE):\n",
    "                idx = slice(start, start + BATCH_SIZE)\n",
    "\n",
    "                # New log-probs & entropy\n",
    "                probs = self.actor(states[idx])\n",
    "                dist = Categorical(probs)\n",
    "                logps = dist.log_prob(actions[idx])\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                # Early stop if KL explodes\n",
    "                kl = (old_logps[idx] - logps).mean()\n",
    "                if kl > 1.5 * CLIP_EPS:\n",
    "                    break\n",
    "\n",
    "                # PPO surrogate\n",
    "                ratio = torch.exp(logps - old_logps[idx])\n",
    "                s1 = ratio * advs[idx]\n",
    "                s2 = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS) * advs[idx]\n",
    "                loss_actor = -(torch.min(s1, s2).mean() + ENTROPY_COEF * entropy)\n",
    "\n",
    "                # Critic loss\n",
    "                vals = self.critic(states[idx]).squeeze()\n",
    "                loss_critic = nn.MSELoss()(vals, rets[idx])\n",
    "\n",
    "                # Step actor\n",
    "                self.opt_actor.zero_grad()\n",
    "                loss_actor.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)\n",
    "                self.opt_actor.step()\n",
    "\n",
    "                # Step critic\n",
    "                self.opt_critic.zero_grad()\n",
    "                loss_critic.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
    "                self.opt_critic.step()\n",
    "\n",
    "\n",
    "# === Potential-Based Shaping ===\n",
    "def phi(state, alpha=10., beta=5.):\n",
    "    pos, vel = state\n",
    "    pos_feat = (pos + 1.2) / 1.8  # normalize position to [0,1]\n",
    "    vel_feat = abs(vel)\n",
    "    return alpha * pos_feat + beta * vel_feat\n",
    "\n",
    "\n",
    "def get_shaped_reward(state, next_state, base_reward):\n",
    "    return base_reward + (GAMMA * phi(next_state) - phi(state))\n",
    "\n",
    "\n",
    "# === State Normalization ===\n",
    "def normalize_state(s):\n",
    "    pos, vel = s\n",
    "    p = (pos - (-1.2)) / (0.6 - (-1.2)) * 2 - 1\n",
    "    v = (vel - (-0.07)) / (0.07 - (-0.07)) * 2 - 1\n",
    "    return np.array([p, v], dtype=np.float32)\n",
    "\n",
    "\n",
    "# === Training Loop ===\n",
    "def train():\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    agent = PPO(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "    scores = deque(maxlen=100)\n",
    "    best_avg = -float('inf')\n",
    "\n",
    "    for ep in range(NUM_EPISODES):\n",
    "        raw_s, _ = env.reset()\n",
    "        s = normalize_state(raw_s)\n",
    "        done = False\n",
    "\n",
    "        states, actions, logps, values, rewards, dones = [], [], [], [], [], []\n",
    "        total_base = 0.0\n",
    "\n",
    "        for t in range(MAX_STEPS):\n",
    "            a, lp, ent = agent.get_action(s)\n",
    "            v = agent.critic(torch.tensor(s).unsqueeze(0)).item()\n",
    "\n",
    "            raw_s2, base_r, done, _, _ = env.step(a)\n",
    "            shaped_r = get_shaped_reward(raw_s, raw_s2, base_r)\n",
    "\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            logps.append(lp)\n",
    "            values.append(v)\n",
    "            rewards.append(shaped_r)\n",
    "            dones.append(done)\n",
    "\n",
    "            raw_s, s = raw_s2, normalize_state(raw_s2)\n",
    "            total_base += base_r\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        advs, rets = agent.compute_adv(rewards, values, dones)\n",
    "        agent.update(states, actions, logps, advs, rets)\n",
    "\n",
    "        scores.append(total_base)\n",
    "        avg_score = np.mean(scores)\n",
    "\n",
    "        print(f'\\rEpisode {ep:4d}\\tAvg Score (base): {avg_score:7.2f}', end='')\n",
    "        if ep % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        # save best\n",
    "        if len(scores) == 100 and avg_score > best_avg:\n",
    "            best_avg = avg_score\n",
    "            torch.save(agent.actor.state_dict(), 'models/ppo_actor_best.pth')\n",
    "            torch.save(agent.critic.state_dict(), 'models/ppo_critic_best.pth')\n",
    "\n",
    "        # solved?\n",
    "        if len(scores) == 100 and avg_score >= -110:\n",
    "            print(f\"\\nSolved at episode {ep - 100}! Avg Score: {avg_score:.2f}\")\n",
    "            torch.save(agent.actor.state_dict(), 'models/ppo_actor_solved.pth')\n",
    "            torch.save(agent.critic.state_dict(), 'models/ppo_critic_solved.pth')\n",
    "            break\n",
    "\n",
    "    # final save\n",
    "    torch.save(agent.actor.state_dict(), 'models/ppo_actor_final.pth')\n",
    "    torch.save(agent.critic.state_dict(), 'models/ppo_critic_final.pth')\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    0\tAvg Score (base): -1000.00\n",
      "Episode   40\tAvg Score (base): -982.200"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Artificial\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100\tAvg Score (base): -559.72\n",
      "Episode  200\tAvg Score (base): -142.72\n",
      "Episode  300\tAvg Score (base): -134.19\n",
      "Episode  400\tAvg Score (base): -130.38\n",
      "Episode  500\tAvg Score (base): -133.18\n",
      "Episode  600\tAvg Score (base): -137.17\n",
      "Episode  700\tAvg Score (base): -129.43\n",
      "Episode  800\tAvg Score (base): -130.22\n",
      "Episode  900\tAvg Score (base): -134.97\n",
      "Episode 1000\tAvg Score (base): -132.81\n",
      "Episode 1100\tAvg Score (base): -126.12\n",
      "Episode 1200\tAvg Score (base): -134.07\n",
      "Episode 1300\tAvg Score (base): -130.31\n",
      "Episode 1400\tAvg Score (base): -126.36\n",
      "Episode 1500\tAvg Score (base): -124.77\n",
      "Episode 1600\tAvg Score (base): -126.01\n",
      "Episode 1700\tAvg Score (base): -127.56\n",
      "Episode 1800\tAvg Score (base): -126.84\n",
      "Episode 1900\tAvg Score (base): -131.53\n",
      "Episode 1999\tAvg Score (base): -129.30"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T13:48:06.223570Z",
     "start_time": "2025-04-25T13:47:56.283365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# Load your trained models\n",
    "def evaluate(model_path_actor, model_path_critic, num_episodes=10, render=False):\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    if render:\n",
    "        env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "    # Initialize agent with same architecture\n",
    "    agent = PPO(env.observation_space.shape[0], env.action_space.n)\n",
    "    agent.actor.load_state_dict(torch.load(model_path_actor))\n",
    "    agent.critic.load_state_dict(torch.load(model_path_critic))\n",
    "    agent.actor.eval()\n",
    "    agent.critic.eval()\n",
    "\n",
    "    success_count = 0\n",
    "    total_rewards = []\n",
    "    steps_to_success = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        raw_s, _ = env.reset()\n",
    "        s = normalize_state(raw_s)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        for t in range(1000):  # Max steps per episode\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                a, _, _ = agent.get_action(s)\n",
    "\n",
    "            raw_s2, r, done, _, _ = env.step(a)\n",
    "            s = normalize_state(raw_s2)\n",
    "            total_reward += r\n",
    "            step_count += 1\n",
    "\n",
    "            if done:\n",
    "                # Check if the car reached the goal (position >= 0.5)\n",
    "                if raw_s2[0] >= 0.5:\n",
    "                    success_count += 1\n",
    "                    steps_to_success.append(step_count)\n",
    "                    print(f\"Episode {ep + 1}: Success! Reached in {step_count} steps\")\n",
    "                else:\n",
    "                    print(f\"Episode {ep + 1}: Failed (position: {raw_s2[0]:.2f})\")\n",
    "                break\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    success_rate = success_count / num_episodes * 100\n",
    "    avg_steps = np.mean(steps_to_success) if steps_to_success else 0\n",
    "\n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    print(f\"Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"Average steps when successful: {avg_steps:.1f}\")\n",
    "    print(f\"Average total reward: {np.mean(total_rewards):.2f}\")\n",
    "\n",
    "    return success_rate, avg_steps\n",
    "\n",
    "\n",
    "# State normalization (same as in training)\n",
    "def normalize_state(s):\n",
    "    pos, vel = s\n",
    "    p = (pos - (-1.2)) / (0.6 - (-1.2)) * 2 - 1\n",
    "    v = (vel - (-0.07)) / (0.07 - (-0.07)) * 2 - 1\n",
    "    return np.array([p, v], dtype=np.float32)\n",
    "\n",
    "\n",
    "# PPO class (same as in training)\n",
    "class PPO:\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        self.actor = Actor(obs_dim, act_dim)\n",
    "        self.critic = Critic(obs_dim)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.as_tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        a = dist.sample()\n",
    "        return a.item(), dist.log_prob(a).detach(), dist.entropy().detach()\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, act_dim), nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Evaluate the best model\n",
    "    print(\"Evaluating best model...\")\n",
    "    evaluate('models/ppo_actor_best.pth', 'models/ppo_critic_best.pth', num_episodes=10)\n",
    "\n",
    "    # Evaluate the solved model (if it exists)\n",
    "    try:\n",
    "        print(\"\\nEvaluating solved model...\")\n",
    "        evaluate('models/ppo_actor_solved.pth', 'models/ppo_critic_solved.pth', num_episodes=10)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Solved model not found - skipping\")\n",
    "\n",
    "    # Evaluate with rendering to visualize one episode\n",
    "    print(\"\\nVisualizing one episode...\")\n",
    "    evaluate('models/ppo_actor_best.pth', 'models/ppo_critic_best.pth', num_episodes=1, render=True)"
   ],
   "id": "4830707d2d52bd22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating best model...\n",
      "Episode 1: Success! Reached in 142 steps\n",
      "Episode 2: Success! Reached in 110 steps\n",
      "Episode 3: Success! Reached in 108 steps\n",
      "Episode 4: Success! Reached in 112 steps\n",
      "Episode 5: Success! Reached in 174 steps\n",
      "Episode 6: Success! Reached in 160 steps\n",
      "Episode 7: Success! Reached in 141 steps\n",
      "Episode 8: Success! Reached in 112 steps\n",
      "Episode 9: Success! Reached in 111 steps\n",
      "Episode 10: Success! Reached in 154 steps\n",
      "\n",
      "=== Evaluation Results ===\n",
      "Success rate: 100.0%\n",
      "Average steps when successful: 132.4\n",
      "Average total reward: -132.40\n",
      "\n",
      "Evaluating solved model...\n",
      "Solved model not found - skipping\n",
      "\n",
      "Visualizing one episode...\n",
      "Episode 1: Success! Reached in 118 steps\n",
      "\n",
      "=== Evaluation Results ===\n",
      "Success rate: 100.0%\n",
      "Average steps when successful: 118.0\n",
      "Average total reward: -118.00\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
