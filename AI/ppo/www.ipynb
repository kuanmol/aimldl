{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T12:01:49.979520800Z",
     "start_time": "2025-04-13T11:58:38.687717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "env = gym.make(\"Humanoid-v5\", render_mode=\"rgb_array\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space\n",
    "state_size, action_size"
   ],
   "id": "6d2c21c50f3d2d63",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(348, Box(-0.4, 0.4, (17,), float32))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T12:01:49.979520800Z",
     "start_time": "2025-04-13T11:58:39.482432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "4eb3bb54a163d4b7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T12:01:49.981530600Z",
     "start_time": "2025-04-13T11:58:41.735929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "# Hyperparameters\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.env_name = \"Humanoid-v4\"\n",
    "        self.seed = 42\n",
    "        self.gamma = 0.99\n",
    "        self.gae_lambda = 0.95\n",
    "        self.entropy_coef = 0.01\n",
    "        self.value_loss_coef = 0.5\n",
    "        self.max_grad_norm = 0.5\n",
    "        self.num_steps = 2048\n",
    "        self.num_envs = 1\n",
    "        self.ppo_epochs = 10\n",
    "        self.num_minibatches = 32\n",
    "        self.clip_param = 0.2\n",
    "        self.lr = 3e-4\n",
    "        self.eps = 1e-5\n",
    "        self.hidden_size = 64\n",
    "        self.max_episodes = 10000\n",
    "        self.save_interval = 100\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)"
   ],
   "id": "bb5aa1ffab7cdc29",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T12:01:49.981530600Z",
     "start_time": "2025-04-13T11:58:41.750484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Actor-Critic Network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # Shared network\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Actor (policy) network\n",
    "        self.actor = nn.Linear(hidden_size, num_actions)\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, num_actions))\n",
    "\n",
    "        # Critic (value) network\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.orthogonal_(m.weight.data)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "    def act(self, state):\n",
    "        mean, value = self.forward(state)\n",
    "        logstd = self.actor_logstd.expand_as(mean)\n",
    "        std = torch.exp(logstd)\n",
    "\n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n",
    "\n",
    "        return action, log_prob, value\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        mean, value = self.forward(state)\n",
    "        logstd = self.actor_logstd.expand_as(mean)\n",
    "        std = torch.exp(logstd)\n",
    "\n",
    "        dist = Normal(mean, std)\n",
    "        log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n",
    "        entropy = dist.entropy().sum(-1, keepdim=True)\n",
    "\n",
    "        return log_prob, value, entropy"
   ],
   "id": "c2c10d9fd4e77116",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T12:01:49.983538800Z",
     "start_time": "2025-04-13T11:58:41.769755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# PPO Agent\n",
    "class PPO:\n",
    "    def __init__(self, num_inputs, num_actions, config):\n",
    "        self.config = config\n",
    "\n",
    "        self.model = ActorCritic(num_inputs, num_actions, config.hidden_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=config.lr, eps=config.eps)\n",
    "\n",
    "        # Storage for rollout data\n",
    "        self.states = torch.zeros(config.num_steps, config.num_envs, num_inputs).to(device)\n",
    "        self.actions = torch.zeros(config.num_steps, config.num_envs, num_actions).to(device)\n",
    "        self.log_probs = torch.zeros(config.num_steps, config.num_envs, 1).to(device)\n",
    "        self.values = torch.zeros(config.num_steps, config.num_envs, 1).to(device)\n",
    "        self.rewards = torch.zeros(config.num_steps, config.num_envs, 1).to(device)\n",
    "        self.masks = torch.zeros(config.num_steps, config.num_envs, 1).to(device)\n",
    "        self.returns = torch.zeros(config.num_steps, config.num_envs, 1).to(device)\n",
    "        self.advantages = torch.zeros(config.num_steps, config.num_envs, 1).to(device)\n",
    "\n",
    "    def compute_gae(self, next_value):\n",
    "        gae = 0\n",
    "        for step in reversed(range(self.config.num_steps)):\n",
    "            if step == self.config.num_steps - 1:\n",
    "                next_non_terminal = 1.0 - self.masks[step]\n",
    "                next_values = next_value\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - self.masks[step + 1]\n",
    "                next_values = self.values[step + 1]\n",
    "\n",
    "            delta = self.rewards[step] + self.config.gamma * next_values * next_non_terminal - self.values[step]\n",
    "            gae = delta + self.config.gamma * self.config.gae_lambda * next_non_terminal * gae\n",
    "            self.advantages[step] = gae\n",
    "\n",
    "        self.returns = self.advantages + self.values\n",
    "\n",
    "    def update(self):\n",
    "        # Flatten the batch\n",
    "        states = self.states.view(-1, self.states.size(-1))\n",
    "        actions = self.actions.view(-1, self.actions.size(-1))\n",
    "        log_probs_old = self.log_probs.view(-1, 1)\n",
    "        returns = self.returns.view(-1, 1)\n",
    "        advantages = self.advantages.view(-1, 1)\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.config.ppo_epochs):\n",
    "            # Minibatch update\n",
    "            for batch_idx in self._get_batches(states.size(0)):\n",
    "                batch_states = states[batch_idx]\n",
    "                batch_actions = actions[batch_idx]\n",
    "                batch_log_probs_old = log_probs_old[batch_idx]\n",
    "                batch_returns = returns[batch_idx]\n",
    "                batch_advantages = advantages[batch_idx]\n",
    "\n",
    "                # Evaluate actions\n",
    "                log_probs, values, entropy = self.model.evaluate(batch_states, batch_actions)\n",
    "\n",
    "                # Compute ratio (pi_theta / pi_theta_old)\n",
    "                ratio = torch.exp(log_probs - batch_log_probs_old)\n",
    "\n",
    "                # Compute surrogate losses\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.config.clip_param, 1.0 + self.config.clip_param) * batch_advantages\n",
    "\n",
    "                # Policy and value losses\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = F.mse_loss(values, batch_returns)\n",
    "                entropy_loss = -entropy.mean()\n",
    "\n",
    "                # Total loss\n",
    "                loss = policy_loss + self.config.value_loss_coef * value_loss + self.config.entropy_coef * entropy_loss\n",
    "\n",
    "                # Gradient step\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def _get_batches(self, num_samples):\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        batch_size = num_samples // self.config.num_minibatches\n",
    "\n",
    "        for i in range(self.config.num_minibatches):\n",
    "            yield indices[i*batch_size : (i+1)*batch_size]"
   ],
   "id": "7d70aa303ddefba",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-13T12:19:47.989629Z",
     "start_time": "2025-04-13T12:12:35.689752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.env_name = \"Humanoid-v5\"  # Updated to v5\n",
    "        self.seed = 42\n",
    "        self.gamma = 0.99\n",
    "        self.gae_lambda = 0.95\n",
    "        self.entropy_coef = 0.01\n",
    "        self.value_loss_coef = 0.5\n",
    "        self.max_grad_norm = 0.5\n",
    "        self.num_steps = 4096  # Increased from 2048\n",
    "        self.num_envs = 1\n",
    "        self.ppo_epochs = 10\n",
    "        self.num_minibatches = 32\n",
    "        self.clip_param = 0.2\n",
    "        self.lr = 1e-4  # Reduced from 3e-4\n",
    "        self.eps = 1e-5\n",
    "        self.hidden_size = 256  # Increased from 64\n",
    "        self.max_episodes = 10000\n",
    "        self.save_interval = 100\n",
    "        self.early_stop_threshold = 3000  # Target reward for early success\n",
    "\n",
    "config = Config()\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=256):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.actor = nn.Linear(hidden_size, num_actions)\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, num_actions))\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.orthogonal_(m.weight.data)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "    def act(self, state):\n",
    "        mean, value = self.forward(state)\n",
    "        logstd = self.actor_logstd.expand_as(mean)\n",
    "        std = torch.exp(logstd)\n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n",
    "        return action, log_prob, value\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        mean, value = self.forward(state)\n",
    "        logstd = self.actor_logstd.expand_as(mean)\n",
    "        std = torch.exp(logstd)\n",
    "        dist = Normal(mean, std)\n",
    "        log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n",
    "        entropy = dist.entropy().sum(-1, keepdim=True)\n",
    "        return log_prob, value, entropy\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, num_inputs, num_actions, config):\n",
    "        self.config = config\n",
    "        self.model = ActorCritic(num_inputs, num_actions, config.hidden_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=config.lr, eps=config.eps)\n",
    "        self.states = torch.zeros(config.num_steps, config.num_envs, num_inputs).to(device)\n",
    "        self.actions = torch.zeros(config.num_steps, config.num_envs, num_actions).to(device)\n",
    "        self.log_probs = torch.zeros(config.num_steps, config.num_envs, 1).to(device)\n",
    "        self.values = torch.zeros(config.num_steps, config.num_envs, 1).to(device)\n",
    "        self.rewards = torch.zeros(config.num_steps, config.num_envs, 1).to(device)\n",
    "        self.masks = torch.zeros(config.num_steps, config.num_envs, 1).to(device)\n",
    "        self.returns = torch.zeros(config.num_steps, config.num_envs, 1).to(device)\n",
    "        self.advantages = torch.zeros(config.num_steps, config.num_envs, 1).to(device)\n",
    "\n",
    "    def compute_gae(self, next_value):\n",
    "        gae = 0\n",
    "        for step in reversed(range(self.config.num_steps)):\n",
    "            if step == self.config.num_steps - 1:\n",
    "                next_non_terminal = 1.0 - self.masks[step]\n",
    "                next_values = next_value\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - self.masks[step + 1]\n",
    "                next_values = self.values[step + 1]\n",
    "            delta = self.rewards[step] + self.config.gamma * next_values * next_non_terminal - self.values[step]\n",
    "            gae = delta + self.config.gamma * self.config.gae_lambda * next_non_terminal * gae\n",
    "            self.advantages[step] = gae\n",
    "        self.returns = self.advantages + self.values\n",
    "\n",
    "    def update(self):\n",
    "        states = self.states.view(-1, self.states.size(-1))\n",
    "        actions = self.actions.view(-1, self.actions.size(-1))\n",
    "        log_probs_old = self.log_probs.view(-1, 1)\n",
    "        returns = self.returns.view(-1, 1)\n",
    "        advantages = self.advantages.view(-1, 1)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)\n",
    "\n",
    "        for _ in range(self.config.ppo_epochs):\n",
    "            for batch_idx in self._get_batches(states.size(0)):\n",
    "                batch_states = states[batch_idx]\n",
    "                batch_actions = actions[batch_idx]\n",
    "                batch_log_probs_old = log_probs_old[batch_idx]\n",
    "                batch_returns = returns[batch_idx]\n",
    "                batch_advantages = advantages[batch_idx]\n",
    "\n",
    "                log_probs, values, entropy = self.model.evaluate(batch_states, batch_actions)\n",
    "                ratio = torch.exp(log_probs - batch_log_probs_old)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.config.clip_param, 1.0 + self.config.clip_param) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = F.mse_loss(values, batch_returns)\n",
    "                entropy_loss = -entropy.mean()\n",
    "                loss = policy_loss + self.config.value_loss_coef * value_loss + self.config.entropy_coef * entropy_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def _get_batches(self, num_samples):\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        batch_size = num_samples // self.config.num_minibatches\n",
    "        for i in range(self.config.num_minibatches):\n",
    "            yield indices[i*batch_size : (i+1)*batch_size]\n",
    "\n",
    "def train():\n",
    "    env = gym.make(config.env_name)\n",
    "    num_inputs = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.shape[0]\n",
    "    agent = PPO(num_inputs, num_actions, config)\n",
    "\n",
    "    # Training metrics\n",
    "    episode_rewards = []\n",
    "    running_reward = 0\n",
    "    best_reward = -np.inf\n",
    "    start_time = time.time()\n",
    "    episode_counter = 0\n",
    "    group_size = 25\n",
    "\n",
    "    # Early stopping\n",
    "    no_improvement = 0\n",
    "    early_stop_patience = 100\n",
    "\n",
    "    # Observation normalization\n",
    "    obs_mean = torch.zeros(num_inputs).to(device)\n",
    "    obs_var = torch.ones(num_inputs).to(device)\n",
    "    obs_count = 1e-4\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Observation space: {num_inputs}, Action space: {num_actions}\")\n",
    "    print(f\"Training for max {config.max_episodes} episodes\")\n",
    "\n",
    "    try:\n",
    "        while episode_counter < config.max_episodes:\n",
    "            group_rewards = []\n",
    "            group_start_time = time.time()\n",
    "\n",
    "            for _ in range(group_size):\n",
    "                episode_reward = 0\n",
    "                for step in range(config.num_steps):\n",
    "                    # Normalize state\n",
    "                    normalized_state = (state - obs_mean) / torch.sqrt(obs_var + 1e-8)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        action, log_prob, value = agent.model.act(normalized_state)\n",
    "\n",
    "                    next_state, reward, terminated, truncated, _ = env.step(action.cpu().numpy()[0])\n",
    "                    done = terminated or truncated\n",
    "                    mask = 1.0 - float(done)\n",
    "                    reward = np.clip(reward, -10, 10)  # Reward clipping\n",
    "\n",
    "                    agent.states[step] = normalized_state\n",
    "                    agent.actions[step] = action\n",
    "                    agent.log_probs[step] = log_prob\n",
    "                    agent.values[step] = value\n",
    "                    agent.rewards[step] = torch.FloatTensor([reward/10.0]).unsqueeze(1).to(device)\n",
    "                    agent.masks[step] = torch.FloatTensor([mask]).unsqueeze(1).to(device)\n",
    "\n",
    "                    state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "                    episode_reward += reward\n",
    "\n",
    "                    if done:\n",
    "                        state, _ = env.reset()\n",
    "                        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                        episode_rewards.append(episode_reward)\n",
    "                        group_rewards.append(episode_reward)\n",
    "                        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "                        episode_counter += 1\n",
    "                        break\n",
    "\n",
    "                # Update normalization stats\n",
    "                with torch.no_grad():\n",
    "                    obs_mean = (obs_count * obs_mean + state.sum(0)) / (obs_count + state.size(0))\n",
    "                    obs_var = (obs_count * obs_var + ((state - obs_mean)**2).sum(0)) / (obs_count + state.size(0))\n",
    "                    obs_count += state.size(0)\n",
    "\n",
    "                # Update policy\n",
    "                with torch.no_grad():\n",
    "                    normalized_state = (state - obs_mean) / torch.sqrt(obs_var + 1e-8)\n",
    "                    _, _, next_value = agent.model.act(normalized_state)\n",
    "                agent.compute_gae(next_value)\n",
    "                agent.update()\n",
    "\n",
    "            # Print group statistics\n",
    "            group_avg = np.mean(group_rewards)\n",
    "            group_std = np.std(group_rewards)\n",
    "            group_min = np.min(group_rewards)\n",
    "            group_max = np.max(group_rewards)\n",
    "            time_per_episode = (time.time() - group_start_time) / group_size\n",
    "\n",
    "            print(f\"\\nEpisodes {episode_counter-group_size+1}-{episode_counter}:\")\n",
    "            print(f\"  Avg: {group_avg:.1f} ± {group_std:.1f}\")\n",
    "            print(f\"  Range: {group_min:.1f} to {group_max:.1f}\")\n",
    "            print(f\"  Running Avg: {running_reward:.1f}\")\n",
    "            print(f\"  Time/episode: {time_per_episode:.2f}s\")\n",
    "\n",
    "            # Save and early stopping\n",
    "            if running_reward > best_reward:\n",
    "                best_reward = running_reward\n",
    "                torch.save(agent.model.state_dict(), \"humanoid_ppo_best.pth\")\n",
    "                no_improvement = 0\n",
    "            else:\n",
    "                no_improvement += group_size\n",
    "\n",
    "            if running_reward >= config.early_stop_threshold:\n",
    "                print(f\"\\nEarly success! Reached target reward {config.early_stop_threshold}\")\n",
    "                break\n",
    "\n",
    "            if no_improvement >= early_stop_patience:\n",
    "                print(f\"\\nEarly stopping - no improvement for {early_stop_patience} episodes\")\n",
    "                break\n",
    "\n",
    "            if episode_counter % config.save_interval == 0:\n",
    "                torch.save(agent.model.state_dict(), f\"humanoid_ppo_{episode_counter}.pth\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "\n",
    "    finally:\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {total_time/3600:.2f} hours\")\n",
    "        print(f\"Final running reward: {running_reward:.1f}\")\n",
    "        print(f\"Best running reward: {best_reward:.1f}\")\n",
    "        env.close()\n",
    "        return episode_rewards\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(config.seed)\n",
    "    np.random.seed(config.seed)\n",
    "    rewards = train()"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Observation space: 348, Action space: 17\n",
      "Training for max 10000 episodes\n",
      "\n",
      "Episodes 1-25:\n",
      "  Avg: -12.3 ± 42.2\n",
      "  Range: -101.2 to 46.6\n",
      "  Running Avg: -19.2\n",
      "  Time/episode: 2.79s\n",
      "\n",
      "Episodes 26-50:\n",
      "  Avg: -167.2 ± 72.8\n",
      "  Range: -324.6 to -59.2\n",
      "  Running Avg: -139.7\n",
      "  Time/episode: 2.82s\n",
      "\n",
      "Episodes 51-75:\n",
      "  Avg: -221.0 ± 53.2\n",
      "  Range: -355.4 to -170.0\n",
      "  Running Avg: -199.5\n",
      "  Time/episode: 3.28s\n",
      "\n",
      "Episodes 76-100:\n",
      "  Avg: -204.0 ± 52.6\n",
      "  Range: -350.0 to -160.0\n",
      "  Running Avg: -206.7\n",
      "  Time/episode: 4.22s\n",
      "\n",
      "Episodes 101-125:\n",
      "  Avg: -222.0 ± 63.3\n",
      "  Range: -420.0 to -160.0\n",
      "  Running Avg: -220.5\n",
      "  Time/episode: 4.18s\n",
      "\n",
      "Early stopping - no improvement for 100 episodes\n",
      "\n",
      "Training completed in 0.12 hours\n",
      "Final running reward: -220.5\n",
      "Best running reward: -19.2\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
