{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn as nn\n",
    "\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 258\n",
    "ACTOR_LR = 1e-4\n",
    "CRITIC_LR = 1e-4\n",
    "HIDDEN = 256\n",
    "ENTROPY_COEF = 0.02\n",
    "NUM_EPISODES = 2000\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "\n",
    "# === Actor & Critic Networks ===\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, act_dim), nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ],
   "id": "921cb0d0492115a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# === PPO Agent ===\n",
    "class PPO:\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        self.actor = Actor(obs_dim, act_dim)\n",
    "        self.critic = Critic(obs_dim)\n",
    "        self.opt_actor = optim.Adam(self.actor.parameters(), lr=ACTOR_LR)\n",
    "        self.opt_critic = optim.Adam(self.critic.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.as_tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        a = dist.sample()\n",
    "        return a.item(), dist.log_prob(a).detach(), dist.entropy().detach()\n",
    "\n",
    "    def compute_adv(self, rewards, values, dones):\n",
    "        advs, rets = [], []\n",
    "        gae, next_val = 0.0, 0.0\n",
    "        for r, v, d in zip(reversed(rewards), reversed(values), reversed(dones)):\n",
    "            delta = r + GAMMA * next_val * (1 - d) - v\n",
    "            gae = delta + GAMMA * LAMBDA * gae * (1 - d)\n",
    "            advs.insert(0, gae)\n",
    "            rets.insert(0, gae + v)\n",
    "            next_val = v\n",
    "        advs = torch.tensor(advs, dtype=torch.float32)\n",
    "        rets = torch.tensor(rets, dtype=torch.float32)\n",
    "        return advs, rets\n",
    "\n",
    "    def update(self, states, actions, old_logps, advs, rets):\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        old_logps = torch.tensor(old_logps, dtype=torch.float32)\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            for start in range(0, len(states), BATCH_SIZE):\n",
    "                idx = slice(start, start + BATCH_SIZE)\n",
    "\n",
    "                # New log-probs & entropy\n",
    "                probs = self.actor(states[idx])\n",
    "                dist = Categorical(probs)\n",
    "                logps = dist.log_prob(actions[idx])\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                # Early stop if KL explodes\n",
    "                kl = (old_logps[idx] - logps).mean()\n",
    "                if kl > 1.5 * CLIP_EPS:\n",
    "                    break\n",
    "\n",
    "                # PPO surrogate\n",
    "                ratio = torch.exp(logps - old_logps[idx])\n",
    "                s1 = ratio * advs[idx]\n",
    "                s2 = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS) * advs[idx]\n",
    "                loss_actor = -(torch.min(s1, s2).mean() + ENTROPY_COEF * entropy)\n",
    "\n",
    "                # Critic loss\n",
    "                vals = self.critic(states[idx]).squeeze()\n",
    "                loss_critic = nn.MSELoss()(vals, rets[idx])\n",
    "\n",
    "                # Step actor\n",
    "                self.opt_actor.zero_grad()\n",
    "                loss_actor.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)\n",
    "                self.opt_actor.step()\n",
    "\n",
    "                # Step critic\n",
    "                self.opt_critic.zero_grad()\n",
    "                loss_critic.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
    "                self.opt_critic.step()"
   ],
   "id": "498c10ac7b713056"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# === Potential-Based Shaping ===\n",
    "def phi(state, alpha=10., beta=5.):\n",
    "    pos, vel = state\n",
    "    pos_feat = (pos + 1.2) / 1.8  # normalize position to [0,1]\n",
    "    vel_feat = abs(vel)\n",
    "    return alpha * pos_feat + beta * vel_feat\n",
    "\n",
    "\n",
    "def get_shaped_reward(state, next_state, base_reward):\n",
    "    return base_reward + (GAMMA * phi(next_state) - phi(state))\n",
    "\n",
    "\n",
    "# === State Normalization ===\n",
    "def normalize_state(s):\n",
    "    pos, vel = s\n",
    "    p = (pos - (-1.2)) / (0.6 - (-1.2)) * 2 - 1\n",
    "    v = (vel - (-0.07)) / (0.07 - (-0.07)) * 2 - 1\n",
    "    return np.array([p, v], dtype=np.float32)\n",
    "\n",
    "\n",
    "# === Training Loop ===\n",
    "def train():\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    agent = PPO(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "    scores = deque(maxlen=100)\n",
    "    best_avg = -float('inf')\n",
    "\n",
    "    for ep in range(NUM_EPISODES):\n",
    "        raw_s, _ = env.reset()\n",
    "        s = normalize_state(raw_s)\n",
    "        done = False\n",
    "\n",
    "        states, actions, logps, values, rewards, dones = [], [], [], [], [], []\n",
    "        total_base = 0.0\n",
    "\n",
    "        for t in range(MAX_STEPS):\n",
    "            a, lp, ent = agent.get_action(s)\n",
    "            v = agent.critic(torch.tensor(s).unsqueeze(0)).item()\n",
    "\n",
    "            raw_s2, base_r, done, _, _ = env.step(a)\n",
    "            shaped_r = get_shaped_reward(raw_s, raw_s2, base_r)\n",
    "\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            logps.append(lp)\n",
    "            values.append(v)\n",
    "            rewards.append(shaped_r)\n",
    "            dones.append(done)\n",
    "\n",
    "            raw_s, s = raw_s2, normalize_state(raw_s2)\n",
    "            total_base += base_r\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        advs, rets = agent.compute_adv(rewards, values, dones)\n",
    "        agent.update(states, actions, logps, advs, rets)\n",
    "\n",
    "        scores.append(total_base)\n",
    "        avg_score = np.mean(scores)\n",
    "\n",
    "        print(f'\\rEpisode {ep:4d}\\tAvg Score (base): {avg_score:7.2f}', end='')\n",
    "        if ep % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        # save best\n",
    "        if len(scores) == 100 and avg_score > best_avg:\n",
    "            best_avg = avg_score\n",
    "            torch.save(agent.actor.state_dict(), 'models/ppo_actor_best.pth')\n",
    "            torch.save(agent.critic.state_dict(), 'models/ppo_critic_best.pth')\n",
    "\n",
    "        # solved?\n",
    "        if len(scores) == 100 and avg_score >= -110:\n",
    "            print(f\"\\nSolved at episode {ep - 100}! Avg Score: {avg_score:.2f}\")\n",
    "            torch.save(agent.actor.state_dict(), 'models/ppo_actor_solved.pth')\n",
    "            torch.save(agent.critic.state_dict(), 'models/ppo_critic_solved.pth')\n",
    "            break\n",
    "\n",
    "    # final save\n",
    "    torch.save(agent.actor.state_dict(), 'models/ppo_actor_final.pth')\n",
    "    torch.save(agent.critic.state_dict(), 'models/ppo_critic_final.pth')\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Load your trained models\n",
    "def evaluate(model_path_actor, model_path_critic, num_episodes=10, render=False):\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    if render:\n",
    "        env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "    # Initialize agent with same architecture\n",
    "    agent = PPO(env.observation_space.shape[0], env.action_space.n)\n",
    "    agent.actor.load_state_dict(torch.load(model_path_actor))\n",
    "    agent.critic.load_state_dict(torch.load(model_path_critic))\n",
    "    agent.actor.eval()\n",
    "    agent.critic.eval()\n",
    "\n",
    "    success_count = 0\n",
    "    total_rewards = []\n",
    "    steps_to_success = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        raw_s, _ = env.reset()\n",
    "        s = normalize_state(raw_s)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        for t in range(1000):  # Max steps per episode\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                a, _, _ = agent.get_action(s)\n",
    "\n",
    "            raw_s2, r, done, _, _ = env.step(a)\n",
    "            s = normalize_state(raw_s2)\n",
    "            total_reward += r\n",
    "            step_count += 1\n",
    "\n",
    "            if done:\n",
    "                # Check if the car reached the goal (position >= 0.5)\n",
    "                if raw_s2[0] >= 0.5:\n",
    "                    success_count += 1\n",
    "                    steps_to_success.append(step_count)\n",
    "                    print(f\"Episode {ep + 1}: Success! Reached in {step_count} steps\")\n",
    "                else:\n",
    "                    print(f\"Episode {ep + 1}: Failed (position: {raw_s2[0]:.2f})\")\n",
    "                break\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    success_rate = success_count / num_episodes * 100\n",
    "    avg_steps = np.mean(steps_to_success) if steps_to_success else 0\n",
    "\n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    print(f\"Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"Average steps when successful: {avg_steps:.1f}\")\n",
    "    print(f\"Average total reward: {np.mean(total_rewards):.2f}\")\n",
    "\n",
    "    return success_rate, avg_steps\n",
    "\n",
    "\n",
    "# State normalization (same as in training)\n",
    "def normalize_state(s):\n",
    "    pos, vel = s\n",
    "    p = (pos - (-1.2)) / (0.6 - (-1.2)) * 2 - 1\n",
    "    v = (vel - (-0.07)) / (0.07 - (-0.07)) * 2 - 1\n",
    "    return np.array([p, v], dtype=np.float32)\n",
    "\n",
    "\n",
    "# PPO class (same as in training)\n",
    "class PPO:\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        self.actor = Actor(obs_dim, act_dim)\n",
    "        self.critic = Critic(obs_dim)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.as_tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        a = dist.sample()\n",
    "        return a.item(), dist.log_prob(a).detach(), dist.entropy().detach()\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, act_dim), nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Evaluate the best model\n",
    "    print(\"Evaluating best model...\")\n",
    "    evaluate('models/ppo_actor_best.pth', 'models/ppo_critic_best.pth', num_episodes=10)\n",
    "\n",
    "    # Evaluate the solved model (if it exists)\n",
    "    try:\n",
    "        print(\"\\nEvaluating solved model...\")\n",
    "        evaluate('models/ppo_actor_solved.pth', 'models/ppo_critic_solved.pth', num_episodes=10)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Solved model not found - skipping\")\n",
    "\n",
    "    # Evaluate with rendering to visualize one episode\n",
    "    print(\"\\nVisualizing one episode...\")\n",
    "    evaluate('models/ppo_actor_best.pth', 'models/ppo_critic_best.pth', num_episodes=1, render=True)"
   ],
   "id": "4830707d2d52bd22",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
