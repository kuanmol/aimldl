{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPSILON = 0.1\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 3e-4\n",
    "MAX_TIMESTEPS = 1000000\n",
    "REWARD_THRESHOLD = -200\n",
    "N_EVAL_EPISODES = 10\n",
    "\n",
    "\n",
    "# Running statistics for reward normalization\n",
    "class RunningStat:\n",
    "    def __init__(self):\n",
    "        self.mean = 0\n",
    "        self.std = 1\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x)\n",
    "        batch_std = np.std(x) + 1e-8\n",
    "        batch_count = len(x)\n",
    "\n",
    "        self.count += batch_count\n",
    "        delta = batch_mean - self.mean\n",
    "        self.mean += delta * batch_count / self.count\n",
    "        delta2 = batch_mean - self.mean\n",
    "        self.std = np.sqrt(self.std ** 2 + (batch_std ** 2 - self.std ** 2) * batch_count / self.count + (\n",
    "                    delta * delta2 * batch_count * (batch_count - 1)) / self.count ** 2)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "\n",
    "# Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, act_dim)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(act_dim) * 0.5)\n",
    "\n",
    "    def forward(self, state):\n",
    "        mean = self.net(state)\n",
    "        std = torch.exp(self.log_std).clamp(min=0.1)\n",
    "        dist = Normal(mean, std)\n",
    "        return dist\n",
    "\n",
    "\n",
    "# Value Network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "\n",
    "# Compute Generalized Advantage Estimation (GAE)\n",
    "def compute_gae(rewards, values, next_value, dones):\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + GAMMA * next_value * (1 - dones[t]) - values[t]\n",
    "        gae = delta + GAMMA * LAMBDA * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "        next_value = values[t]\n",
    "    return advantages\n",
    "\n",
    "\n",
    "# PPO Update\n",
    "def ppo_update(policy, value, optimizer_policy, optimizer_value, states, actions, old_log_probs, returns, advantages):\n",
    "    # Convert lists to NumPy arrays\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    old_log_probs = np.array(old_log_probs)\n",
    "    returns = np.array(returns)\n",
    "    advantages = np.array(advantages)\n",
    "\n",
    "    # Print statistics for debugging\n",
    "    print(\"Advantages mean:\", np.mean(advantages), \"std:\", np.std(advantages))\n",
    "    print(\"Returns mean:\", np.mean(returns), \"std:\", np.std(returns))\n",
    "    print(\"Policy log_std:\", policy.log_std.detach().cpu().numpy())\n",
    "\n",
    "    for _ in range(EPOCHS):\n",
    "        indices = np.random.permutation(len(states))\n",
    "        for start in range(0, len(states), BATCH_SIZE):\n",
    "            batch_indices = indices[start:start + BATCH_SIZE]\n",
    "            batch_states = torch.tensor(states[batch_indices], dtype=torch.float32).to(device)\n",
    "            batch_actions = torch.tensor(actions[batch_indices], dtype=torch.float32).to(device)\n",
    "            batch_old_log_probs = torch.tensor(old_log_probs[batch_indices], dtype=torch.float32).to(device)\n",
    "            batch_returns = torch.tensor(returns[batch_indices], dtype=torch.float32).to(device)\n",
    "            batch_advantages = torch.tensor(advantages[batch_indices], dtype=torch.float32).to(device)\n",
    "\n",
    "            # Normalize advantages\n",
    "            batch_advantages = (batch_advantages - batch_advantages.mean()) / (batch_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy update\n",
    "            dist = policy(batch_states)\n",
    "            new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "            ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "            surr1 = ratio * batch_advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - CLIP_EPSILON, 1 + CLIP_EPSILON) * batch_advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            optimizer_policy.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)  # Gradient clipping\n",
    "            optimizer_policy.step()\n",
    "\n",
    "            # Value update\n",
    "            value_pred = value(batch_states).squeeze()\n",
    "            value_loss = (value_pred - batch_returns).pow(2).mean()\n",
    "\n",
    "            optimizer_value.zero_grad()\n",
    "            value_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(value.parameters(), max_norm=0.5)  # Gradient clipping\n",
    "            optimizer_value.step()\n",
    "\n",
    "\n",
    "# Evaluate the policy\n",
    "def evaluate_policy(env, policy, n_episodes):\n",
    "    total_rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            dist = policy(state_tensor)\n",
    "            action = dist.sample()\n",
    "            action_clamped = torch.clamp(action, -2.0, 2.0)\n",
    "            state, reward, terminated, truncated, _ = env.step(action_clamped.detach().cpu().numpy()[0])\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "        total_rewards.append(episode_reward)\n",
    "    return np.mean(total_rewards)\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "def train():\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    policy = PolicyNetwork(obs_dim, act_dim).to(device)\n",
    "    value = ValueNetwork(obs_dim).to(device)\n",
    "    optimizer_policy = optim.Adam(policy.parameters(), lr=LEARNING_RATE)\n",
    "    optimizer_value = optim.Adam(value.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    reward_stat = RunningStat()\n",
    "    state, _ = env.reset()\n",
    "    total_steps = 0\n",
    "    episode_rewards = []\n",
    "\n",
    "    while total_steps < MAX_TIMESTEPS:\n",
    "        states, actions, rewards, values, old_log_probs, dones = [], [], [], [], [], []\n",
    "        episode_reward = 0\n",
    "        raw_rewards = []  # For running stat update\n",
    "\n",
    "        # Collect trajectory\n",
    "        for _ in range(200):  # Max episode length\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            dist = policy(state_tensor)\n",
    "            action = dist.sample()\n",
    "            action_clamped = torch.clamp(action, -2.0, 2.0)\n",
    "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "            value_pred = value(state_tensor)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action_clamped.detach().cpu().numpy()[0])\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            raw_rewards.append(reward)\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action_clamped.detach().cpu().numpy()[0])\n",
    "            rewards.append(reward_stat.normalize(reward))  # Normalize reward\n",
    "            values.append(value_pred.item())\n",
    "            old_log_probs.append(log_prob.item())\n",
    "            dones.append(done)\n",
    "\n",
    "            state = next_state\n",
    "            total_steps += 1\n",
    "\n",
    "            if done:\n",
    "                state, _ = env.reset()\n",
    "                episode_rewards.append(episode_reward)\n",
    "                reward_stat.update(raw_rewards)  # Update running stats\n",
    "                episode_reward = 0\n",
    "                raw_rewards = []\n",
    "                break\n",
    "\n",
    "        # If episode didn't terminate, update reward stats\n",
    "        if raw_rewards:\n",
    "            reward_stat.update(raw_rewards)\n",
    "\n",
    "        # Compute returns and advantages\n",
    "        next_state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        next_value = value(next_state_tensor).item()\n",
    "        advantages = compute_gae(rewards, values, next_value, dones)\n",
    "        returns = [adv + val for adv, val in zip(advantages, values)]\n",
    "\n",
    "        # Update policy and value networks\n",
    "        ppo_update(policy, value, optimizer_policy, optimizer_value, states, actions, old_log_probs, returns,\n",
    "                   advantages)\n",
    "\n",
    "        # Evaluate and check stopping condition\n",
    "        if len(episode_rewards) >= N_EVAL_EPISODES:\n",
    "            avg_reward = np.mean(episode_rewards[-N_EVAL_EPISODES:])\n",
    "            print(f\"Steps: {total_steps}, Avg Reward: {avg_reward:.2f}\")\n",
    "            if avg_reward > REWARD_THRESHOLD:\n",
    "                print(f\"Goal reached! Average reward {avg_reward:.2f} exceeds threshold {REWARD_THRESHOLD}\")\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "    torch.save(policy.state_dict(), \"ppo_policy.pth\")\n",
    "    torch.save(value.state_dict(), \"ppo_value.pth\")\n",
    "    print(\"Models saved successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Load your policy network\n",
    "policy = PolicyNetwork(obs_dim, act_dim).to(device)\n",
    "policy.load_state_dict(torch.load(\"ppo_policy.pth\"))\n",
    "policy.eval()\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"Pendulum-v1\", render_mode=\"human\")  # If you want to see\n",
    "# env = gym.make(\"Pendulum-v1\")  # No render if you just want numbers\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "SEED = 42\n",
    "env.reset(seed=SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "N_EVAL_EPISODES = 10\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(N_EVAL_EPISODES):\n",
    "    state, _ = env.reset(seed=SEED + episode)  # different seed per episode for variety but still reproducible\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dist = policy(state_tensor)\n",
    "            action = dist.mean  # <-- use mean action for deterministic behavior\n",
    "            action_clamped = torch.clamp(action, -2.0, 2.0)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action_clamped.squeeze(0).cpu().numpy())\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    print(f\"Episode {episode + 1}: Reward = {total_reward:.2f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\n=== Final Evaluation ===\")\n",
    "print(f\"Average Reward over {N_EVAL_EPISODES} episodes: {np.mean(episode_rewards):.2f}\")\n",
    "print(f\"Reward Std Deviation: {np.std(episode_rewards):.2f}\")\n"
   ],
   "id": "4f29a62e3248a758",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
