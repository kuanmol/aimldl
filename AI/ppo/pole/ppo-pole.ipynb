{
 "cells": [
  {
   "cell_type": "code",
   "id": "edee3e3f26a80d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T12:38:17.678246Z",
     "start_time": "2025-04-25T12:38:17.666689Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "ACTOR_LR = 3e-4\n",
    "CRITIC_LR = 1e-3\n",
    "HIDDEN = 128\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, act_dim), nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        self.actor = Actor(obs_dim, act_dim)\n",
    "        self.critic = Critic(obs_dim)\n",
    "        self.opt_actor = optim.Adam(self.actor.parameters(), lr=ACTOR_LR)\n",
    "        self.opt_critic = optim.Adam(self.critic.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "    def compute_adv(self, rewards, values, dones):\n",
    "        adv, ret, gae, next_val = [], [], 0, 0\n",
    "        for r, v, d in zip(reversed(rewards), reversed(values), reversed(dones)):\n",
    "            delta = r + GAMMA * next_val * (1 - d) - v\n",
    "            gae = delta + GAMMA * LAMBDA * gae * (1 - d)\n",
    "            adv.insert(0, gae)\n",
    "            ret.insert(0, gae + v)\n",
    "            next_val = v\n",
    "        return torch.tensor(adv), torch.tensor(ret)\n",
    "\n",
    "    def update(self, states, actions, old_logps, advs, rets):\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        actions = torch.tensor(actions)\n",
    "        old_logps = torch.tensor(old_logps)\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            for i in range(0, len(states), BATCH_SIZE):\n",
    "                idx = slice(i, i + BATCH_SIZE)\n",
    "                logits = self.actor(states[idx])\n",
    "                dist = Categorical(logits)\n",
    "                logps = dist.log_prob(actions[idx])\n",
    "                ratio = torch.exp(logps - old_logps[idx])\n",
    "                s1 = ratio * advs[idx]\n",
    "                s2 = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS) * advs[idx]\n",
    "                loss_actor = -torch.min(s1, s2).mean()\n",
    "\n",
    "                vals = self.critic(states[idx]).squeeze()\n",
    "                loss_critic = nn.MSELoss()(vals, rets[idx])\n",
    "\n",
    "                self.opt_actor.zero_grad()\n",
    "                loss_actor.backward()\n",
    "                self.opt_actor.step()\n",
    "\n",
    "                self.opt_critic.zero_grad()\n",
    "                loss_critic.backward()\n",
    "                self.opt_critic.step()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-25T12:38:37.081922Z",
     "start_time": "2025-04-25T12:38:19.850047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import imageio\n",
    "import torch\n",
    "\n",
    "def train():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    agent = PPO(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "    for ep in range(1000):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        logps, vals, rewards, states, actions, dones = [], [], [], [], [], []\n",
    "        total = 0\n",
    "\n",
    "        while not done:\n",
    "            action, logp = agent.get_action(state)\n",
    "            value = agent.critic(torch.tensor(state, dtype=torch.float32).unsqueeze(0)).item()\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            logps.append(logp)\n",
    "            vals.append(value)\n",
    "            dones.append(done)\n",
    "\n",
    "            state = next_state\n",
    "            total += reward\n",
    "\n",
    "        advs, rets = agent.compute_adv(rewards, vals, dones)\n",
    "        agent.update(states, actions, logps, advs, rets)\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            print(f\"Episode {ep}, Reward: {total}\")\n",
    "        if total >= 475:\n",
    "            print(f\"Solved at episode {ep}\")\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return agent\n",
    "\n",
    "def record_video(agent, path=\"ppo_cartpole.mp4\", max_steps=1000):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    frames = []\n",
    "    state, _ = env.reset()\n",
    "    for _ in range(max_steps):\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        action, _ = agent.get_action(state)\n",
    "        state, _, done, _, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    imageio.mimsave(path, frames, fps=30)\n",
    "    print(f\"Saved video to {path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = train()\n",
    "    record_video(agent)\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Artificial\\.venv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: 11.0\n",
      "Episode 10, Reward: 38.0\n",
      "Episode 20, Reward: 15.0\n",
      "Episode 30, Reward: 126.0\n",
      "Episode 40, Reward: 338.0\n",
      "Episode 50, Reward: 225.0\n",
      "Solved at episode 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video to ppo_cartpole.mp4\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d7ad5fd79b3c7ece",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
