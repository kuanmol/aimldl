{
 "cells": [
  {
   "cell_type": "code",
   "id": "edee3e3f26a80d6",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "ACTOR_LR = 3e-4\n",
    "CRITIC_LR = 1e-3\n",
    "HIDDEN = 128\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, act_dim), nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, HIDDEN), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        self.actor = Actor(obs_dim, act_dim)\n",
    "        self.critic = Critic(obs_dim)\n",
    "        self.opt_actor = optim.Adam(self.actor.parameters(), lr=ACTOR_LR)\n",
    "        self.opt_critic = optim.Adam(self.critic.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "    def compute_adv(self, rewards, values, dones):\n",
    "        adv, ret, gae, next_val = [], [], 0, 0\n",
    "        for r, v, d in zip(reversed(rewards), reversed(values), reversed(dones)):\n",
    "            delta = r + GAMMA * next_val * (1 - d) - v\n",
    "            gae = delta + GAMMA * LAMBDA * gae * (1 - d)\n",
    "            adv.insert(0, gae)\n",
    "            ret.insert(0, gae + v)\n",
    "            next_val = v\n",
    "        return torch.tensor(adv), torch.tensor(ret)\n",
    "\n",
    "    def update(self, states, actions, old_logps, advs, rets):\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        actions = torch.tensor(actions)\n",
    "        old_logps = torch.tensor(old_logps)\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            for i in range(0, len(states), BATCH_SIZE):\n",
    "                idx = slice(i, i + BATCH_SIZE)\n",
    "                logits = self.actor(states[idx])\n",
    "                dist = Categorical(logits)\n",
    "                logps = dist.log_prob(actions[idx])\n",
    "                ratio = torch.exp(logps - old_logps[idx])\n",
    "                s1 = ratio * advs[idx]\n",
    "                s2 = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS) * advs[idx]\n",
    "                loss_actor = -torch.min(s1, s2).mean()\n",
    "\n",
    "                vals = self.critic(states[idx]).squeeze()\n",
    "                loss_critic = nn.MSELoss()(vals, rets[idx])\n",
    "\n",
    "                self.opt_actor.zero_grad()\n",
    "                loss_actor.backward()\n",
    "                self.opt_actor.step()\n",
    "\n",
    "                self.opt_critic.zero_grad()\n",
    "                loss_critic.backward()\n",
    "                self.opt_critic.step()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import imageio\n",
    "import torch\n",
    "\n",
    "def train():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    agent = PPO(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "    for ep in range(1000):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        logps, vals, rewards, states, actions, dones = [], [], [], [], [], []\n",
    "        total = 0\n",
    "\n",
    "        while not done:\n",
    "            action, logp = agent.get_action(state)\n",
    "            value = agent.critic(torch.tensor(state, dtype=torch.float32).unsqueeze(0)).item()\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            logps.append(logp)\n",
    "            vals.append(value)\n",
    "            dones.append(done)\n",
    "\n",
    "            state = next_state\n",
    "            total += reward\n",
    "\n",
    "        advs, rets = agent.compute_adv(rewards, vals, dones)\n",
    "        agent.update(states, actions, logps, advs, rets)\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            print(f\"Episode {ep}, Reward: {total}\")\n",
    "        if total >= 475:\n",
    "            print(f\"Solved at episode {ep}\")\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return agent\n",
    "\n",
    "def record_video(agent, path=\"ppo_cartpole.mp4\", max_steps=1000):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    frames = []\n",
    "    state, _ = env.reset()\n",
    "    for _ in range(max_steps):\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        action, _ = agent.get_action(state)\n",
    "        state, _, done, _, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    imageio.mimsave(path, frames, fps=30)\n",
    "    print(f\"Saved video to {path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = train()\n",
    "    record_video(agent)\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d7ad5fd79b3c7ece",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T03:17:27.309360Z",
     "start_time": "2025-04-18T03:17:12.119593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "ENV_NAME = \"HumanoidStandup-v4\"\n",
    "MODEL_PATH = r\"D:\\Artificial\\AI\\ppo\\human\\ppo_humanoidstandup_final.pth\"\n",
    "NORMALIZER_PATH = MODEL_PATH + \".norm\"\n",
    "VIDEO_DIR = \"success_videos\"\n",
    "MAX_EPISODE_STEPS = 1000\n",
    "CONTROL_PENALTY = 0.01  # Same as training\n",
    "\n",
    "# Make sure the directory exists\n",
    "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "\n",
    "# Observation Normalizer Class (same as training)\n",
    "class RunningStat:\n",
    "    def __init__(self, shape):\n",
    "        self.mean = np.zeros(shape, dtype=np.float32)\n",
    "        self.std = np.ones(shape, dtype=np.float32)\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_std = np.std(x, axis=0) + 1e-6\n",
    "        batch_count = x.shape[0]\n",
    "        self.count += batch_count\n",
    "        delta = batch_mean - self.mean\n",
    "        self.mean += delta * batch_count / self.count\n",
    "        m_a = self.std * self.std * (self.count - batch_count)\n",
    "        m_b = batch_std * batch_std * batch_count\n",
    "        self.std = np.sqrt((m_a + m_b + np.square(delta) * self.count * batch_count / self.count) / self.count)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return np.clip((x - self.mean) / self.std, -5, 5)\n",
    "\n",
    "# Actor Network (must match training)\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_dim, 256),\n",
    "            torch.nn.LayerNorm(256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 256),\n",
    "            torch.nn.LayerNorm(256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, act_dim * 2)\n",
    "        )\n",
    "        torch.nn.init.orthogonal_(self.net[-1].weight, gain=0.01)\n",
    "        torch.nn.init.zeros_(self.net[-1].bias)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.net(state)\n",
    "        mean, log_std = x.chunk(2, dim=-1)\n",
    "        log_std = torch.clamp(log_std, -10, 2)\n",
    "        return mean, log_std\n",
    "\n",
    "    def get_action(self, state, deterministic=False):\n",
    "        mean, log_std = self.forward(state)\n",
    "        if deterministic:\n",
    "            return torch.tanh(mean)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        return torch.tanh(action)\n",
    "\n",
    "# Evaluation + Video Recording\n",
    "def evaluate():\n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "    env = gym.wrappers.RecordVideo(\n",
    "        env,\n",
    "        video_folder=VIDEO_DIR,\n",
    "        name_prefix=\"success_video\",\n",
    "        episode_trigger=lambda episode_id: True\n",
    "    )\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load Actor model\n",
    "    actor = Actor(obs_dim, act_dim).to(device)\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "    actor.load_state_dict(checkpoint['actor'])\n",
    "    actor.eval()\n",
    "\n",
    "    # Load the full normalizer object\n",
    "    with open(NORMALIZER_PATH, 'rb') as f:\n",
    "        normalizer = pickle.load(f)\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for t in range(MAX_EPISODE_STEPS):\n",
    "        norm_state = normalizer.normalize(state)\n",
    "        state_tensor = torch.FloatTensor(norm_state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            action = actor.get_action(state_tensor, deterministic=True)\n",
    "        action_np = action.cpu().numpy()[0]\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action_np)\n",
    "        reward -= CONTROL_PENALTY * np.sum(action_np ** 2)\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    print(f\"ðŸŽ¥ Success video saved â€” total reward: {episode_reward:.2f}\")\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate()\n"
   ],
   "id": "933dfded992ca4b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Artificial\\.venv\\Lib\\site-packages\\gymnasium\\envs\\registration.py:519: DeprecationWarning: \u001B[33mWARN: The environment HumanoidStandup-v4 is out of date. You should consider upgrading to version `v5`.\u001B[0m\n",
      "  logger.deprecation(\n",
      "D:\\Artificial\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001B[33mWARN: Overwriting existing videos at D:\\Artificial\\AI\\ppo\\pole\\success_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ Success video saved â€” total reward: 28584.71\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
